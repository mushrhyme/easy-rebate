"""
ê²€ìƒ‰ API
"""
import asyncio
import json
from pathlib import Path
from typing import List, Optional

import pandas as pd
from urllib.parse import quote
from fastapi import APIRouter, HTTPException, Depends, Query, Body
from pydantic import BaseModel

from database.registry import get_db
from database.db_manager import _similarity_difflib
from backend.core.auth import get_current_user_optional, get_current_user
from backend.unit_price_lookup import split_name_and_capacity, find_similar_products
from modules.core.rag_manager import get_rag_manager

router = APIRouter()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ (search.py: backend/api/routes/search.py -> parent*4 = project_root)
_PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent.parent
_UNIT_PRICE_CSV = _PROJECT_ROOT / "database" / "csv" / "unit_price.csv"
_RETAIL_USER_CSV = _PROJECT_ROOT / "database" / "csv" / "retail_user.csv"
_DIST_RETAIL_CSV = _PROJECT_ROOT / "database" / "csv" / "dist_retail.csv"
_DOMAE_RETAIL_1_CSV = _PROJECT_ROOT / "database" / "csv" / "domae_retail_1.csv"
_DOMAE_RETAIL_2_CSV = _PROJECT_ROOT / "database" / "csv" / "domae_retail_2.csv"
_SAP_RETAIL_CSV = _PROJECT_ROOT / "database" / "csv" / "sap_retail.csv"
_SAP_PRODUCT_CSV = _PROJECT_ROOT / "database" / "csv" / "sap_product.csv"


# ----- sap_retail ê²€ìƒ‰ (ì •ì  ê²½ë¡œë¥¼ ë§¨ ìœ„ì— ë“±ë¡í•´ 404 ë°©ì§€) -----
@router.get("/retail/candidates-by-sap-retail")
async def get_retail_candidates_by_sap_retail(
    query: str = Query("", description="æ¤œç´¢èªï¼ˆì†Œë§¤ì²˜ëª…ãƒ»íŒë§¤ì²˜ëª… ë“±)"),
    top_k: int = Query(10, ge=1, le=30, description="ë°˜í™˜ ê±´ìˆ˜"),
    min_similarity: float = Query(0.0, ge=0.0, le=1.0, description="ìµœì†Œ ìœ ì‚¬ë„"),
):
    """sap_retail.csvì—ì„œ ì†Œë§¤ì²˜ëª…Â·íŒë§¤ì²˜ëª… ìœ ì‚¬ë„ ê²€ìƒ‰. ë§¤í•‘ í´ë°±ìš©."""
    query = (query or "").strip()
    csv_path = _SAP_RETAIL_CSV
    print(f"[sap_retail] query={query!r} (len={len(query)}), path={csv_path}, exists={csv_path.exists()}")
    if not query:
        return {"query": "", "matches": []}
    if not csv_path.exists():
        return {"query": query, "matches": [], "skipped_reason": "sap_retail.csv not found"}
    try:
        df = pd.read_csv(csv_path, dtype=str, encoding="utf-8-sig")
        df.columns = [c.strip().lstrip("\ufeff") for c in df.columns]
        scored = []
        q_upper = query.upper()
        for _, r in df.iterrows():
            retail_name = (r.get("ì†Œë§¤ì²˜ëª…") or "").strip()
            dist_name = (r.get("íŒë§¤ì²˜ëª…") or "").strip()
            retail_code = (r.get("ì†Œë§¤ì²˜ì½”ë“œ") or "").strip()
            dist_code = (r.get("íŒë§¤ì²˜ì½”ë“œ") or "").strip()
            if not retail_code:
                continue
            s1 = _similarity_difflib(query, retail_name)
            s2 = _similarity_difflib(query, dist_name) if dist_name else 0.0
            # ë¶€ë¶„ ì¼ì¹˜: ê²€ìƒ‰ì–´ê°€ ì´ë¦„ì— í¬í•¨ë˜ë©´ ë†’ì€ ì ìˆ˜ (ì§§ì€ ê²€ìƒ‰ì–´ë„ ë‚˜ì˜¤ë„ë¡)
            part = 0.0
            if query in retail_name or query in dist_name:
                part = 0.95
            elif q_upper in (retail_name.upper() if retail_name else ""):
                part = 0.9
            elif q_upper in (dist_name.upper() if dist_name else ""):
                part = 0.9
            score = max(s1, s2, part)
            if score >= min_similarity:
                scored.append((score, retail_code, retail_name, dist_code, dist_name))
        scored.sort(key=lambda x: -x[0])
        matches = [
            {
                "ì†Œë§¤ì²˜ì½”ë“œ": rc,
                "ì†Œë§¤ì²˜ëª…": rn,
                "íŒë§¤ì²˜ì½”ë“œ": dc,
                "íŒë§¤ì²˜ëª…": dn,
                "similarity": round(sc, 4),
            }
            for sc, rc, rn, dc, dn in scored[:top_k]
        ]
        return {"query": query, "matches": matches}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/retail/sap-row-by-retail-code")
async def get_sap_row_by_retail_code(
    retail_code: str = Query(..., description="ì†Œë§¤ì²˜ì½”ë“œ(å°å£²å…ˆCD)"),
):
    """sap_retailì—ì„œ ì†Œë§¤ì²˜ì½”ë“œë¡œ ì²« í–‰ ì¡°íšŒ. SAPå—æ³¨å…ˆãƒ»SAPå°å£²å…ˆ í‘œì‹œìš©."""
    code = (retail_code or "").strip()
    if not code:
        return {"retail_code": "", "row": None}
    if not _SAP_RETAIL_CSV.exists():
        return {"retail_code": code, "row": None, "skipped_reason": "sap_retail.csv not found"}
    try:
        df = pd.read_csv(_SAP_RETAIL_CSV, dtype=str, encoding="utf-8-sig")
        df.columns = [c.strip().lstrip("\ufeff") for c in df.columns]
        rows = df[df["ì†Œë§¤ì²˜ì½”ë“œ"].astype(str).str.strip() == code]
        if rows.empty:
            return {"retail_code": code, "row": None}
        r = rows.iloc[0]
        return {
            "retail_code": code,
            "row": {
                "ì†Œë§¤ì²˜ì½”ë“œ": (r.get("ì†Œë§¤ì²˜ì½”ë“œ") or "").strip(),
                "ì†Œë§¤ì²˜ëª…": (r.get("ì†Œë§¤ì²˜ëª…") or "").strip(),
                "íŒë§¤ì²˜ì½”ë“œ": (r.get("íŒë§¤ì²˜ì½”ë“œ") or "").strip(),
                "íŒë§¤ì²˜ëª…": (r.get("íŒë§¤ì²˜ëª…") or "").strip(),
            },
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


def _get_in_vector_pdf_filenames(db) -> List[str]:
    """rag_vector_indexì—ì„œ ë²¡í„° ì¸ë±ìŠ¤ì— í¬í•¨ëœ pdf_filename ëª©ë¡ ë°˜í™˜."""
    try:
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT metadata_json FROM rag_vector_index
                WHERE index_name = 'base' AND (form_type IS NULL OR form_type = '')
                ORDER BY updated_at DESC LIMIT 1
            """)
            row = cursor.fetchone()
        if not row:
            return []
        meta = row[0]
        if meta is None:
            return []
        if isinstance(meta, str):
            try:
                meta = json.loads(meta)
            except Exception:
                return []
        if not isinstance(meta, dict):
            return []
        metadata_dict = meta.get("metadata") or meta.get("Metadata") or {}
        pdf_names = set()
        for _doc_id, doc_data in (metadata_dict or {}).items():
            if not isinstance(doc_data, dict):
                continue
            inner = doc_data.get("metadata") or doc_data.get("Metadata") or {}
            pn = (inner or {}).get("pdf_name") or (inner or {}).get("pdf_filename")
            if pn:
                pdf_names.add(pn)
        return [
            p if (p and str(p).lower().endswith(".pdf")) else f"{p}.pdf"
            for p in pdf_names
        ]
    except Exception:
        return []


class PagesByCustomersRequest(BaseModel):
    customer_names: List[str]
    form_type: Optional[str] = None


class CustomerSimilarityMappingRequest(BaseModel):
    """ê±°ë˜ì²˜(ì™¼ìª½) vs ë‹´ë‹¹(ì˜¤ë¥¸ìª½) ìœ ì‚¬ë„ ë§¤í•‘ ìš”ì²­. notepad find_similar_supersì™€ ë™ì¼í•œ difflib ì‚¬ìš©."""
    customer_names: List[str]
    super_names: List[str]


@router.get("/customer")
async def search_by_customer(
    customer_name: str = Query(..., description="ê±°ë˜ì²˜ëª…"),
    exact_match: bool = Query(False, description="ì™„ì „ ì¼ì¹˜ ì—¬ë¶€"),
    form_type: Optional[str] = Query(None, description="ì–‘ì‹ì§€ íƒ€ì… í•„í„°"),
    my_supers_only: bool = Query(False, description="ë¡œê·¸ì¸ ì‚¬ìš©ì ë‹´ë‹¹ ìŠˆí¼ë§Œ (ìœ ì‚¬ë„ 90% ì´ìƒ)"),
    db=Depends(get_db),
    current_user=Depends(get_current_user_optional),
):
    """
    ê±°ë˜ì²˜ëª…ìœ¼ë¡œ ê²€ìƒ‰.
    my_supers_only=Trueì´ë©´ ë¡œê·¸ì¸ í•„ìš”, ë‹´ë‹¹ ìŠˆí¼ëª…ê³¼ ìœ ì‚¬ë„ 90% ì´ìƒì¸ í•­ëª©ë§Œ ë°˜í™˜.
    """
    if my_supers_only and not current_user:
        raise HTTPException(status_code=401, detail="ë‚´ ë‹´ë‹¹ë§Œ ë³´ë ¤ë©´ ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤")
    super_names: Optional[List[str]] = None
    if my_supers_only and current_user:
        from modules.utils.retail_user_utils import get_super_names_for_username
        super_names = get_super_names_for_username(current_user["username"] or "")
        if not super_names:
            return {"query": customer_name, "total_items": 0, "total_pages": 0, "pages": []}
    try:
        results = db.search_items_by_customer(
            customer_name=customer_name,
            exact_match=exact_match,
            form_type=form_type,
            super_names=super_names,
            min_similarity=0.9,
        )
        print(f"ğŸ” [search/customer] query={customer_name!r}, my_supers_only={my_supers_only}, items ê²°ê³¼={len(results)}ê±´")
        
        # íŒŒì¼ëª…ê³¼ í˜ì´ì§€ë³„ë¡œ ê·¸ë£¹í™”
        grouped_results = {}
        for item in results:
            pdf_filename = item.get('pdf_filename')
            page_number = item.get('page_number')
            key = (pdf_filename, page_number)
            
            if key not in grouped_results:
                grouped_results[key] = {
                    'pdf_filename': pdf_filename,
                    'page_number': page_number,
                    'items': [],
                    'form_type': item.get('form_type')
                }
            grouped_results[key]['items'].append(item)
        
        # items ê²€ìƒ‰ ê²°ê³¼ê°€ 0ì´ë©´ page_data.page_meta(JSON í…ìŠ¤íŠ¸)ì—ì„œ í´ë°± ê²€ìƒ‰
        if len(results) == 0 and customer_name.strip():
            fallback_pages = db.search_pages_by_customer_in_page_meta(customer_name.strip())
            print(f"ğŸ” [search/customer] items 0ê±´ â†’ page_meta í´ë°± ê²€ìƒ‰: {len(fallback_pages)}í˜ì´ì§€")
            for row in fallback_pages:
                pdf_filename = row.get('pdf_filename')
                page_number = row.get('page_number')
                if not pdf_filename or not page_number:
                    continue
                key = (pdf_filename, page_number)
                if key in grouped_results:
                    continue
                page_result = db.get_page_result(pdf_filename, page_number)
                if page_result and page_result.get('items'):
                    grouped_results[key] = {
                        'pdf_filename': pdf_filename,
                        'page_number': page_number,
                        'items': page_result['items'],
                        'form_type': row.get('form_type') or (page_result.get('form_type') if isinstance(page_result.get('form_type'), str) else None)
                    }
        
        total_items = sum(len(g['items']) for g in grouped_results.values())
        return {
            "query": customer_name,
            "total_items": total_items,
            "total_pages": len(grouped_results),
            "pages": list(grouped_results.values())
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/my-supers")
async def get_my_supers(current_user=Depends(get_current_user)):
    """
    ë¡œê·¸ì¸ ì‚¬ìš©ì ë‹´ë‹¹ ê±°ë˜ì²˜(ìŠˆí¼) ëª©ë¡ (retail_user.csv ê¸°ì¤€).
    ê²€í†  íƒ­ì—ì„œ ê±°ë˜ì²˜ ëª©ë¡ ë²„íŠ¼ í´ë¦­ ì‹œ "ë‚´ ë‹´ë‹¹ ê±°ë˜ì²˜" í‘œì‹œìš©.
    """
    from modules.utils.retail_user_utils import get_super_names_for_username
    super_names = get_super_names_for_username(current_user["username"] or "")
    return {"super_names": super_names}


@router.get("/all-super-names")
async def get_all_super_names_route(current_user=Depends(get_current_user)):
    """
    retail_user.csv ì†Œë§¤ì²˜ëª… ì „ì²´(ì¤‘ë³µ ì œê±°). ê±°ë˜ì²˜â†”ë‹´ë‹¹ ìœ ì‚¬ë„ ë§¤í•‘ ì‹œ notepadì™€ ë™ì¼í•˜ê²Œ ì „ì²´ í’€ì—ì„œ ìµœì  ë§¤ì¹­ìš©.
    """
    from modules.utils.retail_user_utils import get_all_super_names
    return {"super_names": get_all_super_names()}


@router.get("/my-super-pages")
async def get_my_super_pages(
    form_type: Optional[str] = Query(None, description="ì–‘ì‹ì§€ íƒ€ì… í•„í„°"),
    db=Depends(get_db),
    current_user=Depends(get_current_user),
):
    """
    ë¡œê·¸ì¸ ì‚¬ìš©ì ë‹´ë‹¹ ìŠˆí¼(ê±°ë˜ì²˜)ì— í•´ë‹¹í•˜ëŠ” í•­ëª©ì´ ìˆëŠ” í˜ì´ì§€ ëª©ë¡.
    retail_user.csv ê¸°ì¤€, ê²€í†  íƒ­ "ë‚´ ë‹´ë‹¹ë§Œ" í•„í„°ìš© (ìœ ì‚¬ë„ 90% ì´ìƒ).
    """
    from modules.utils.retail_user_utils import get_super_names_for_username
    super_names = get_super_names_for_username(current_user["username"] or "")
    pages = db.get_page_keys_by_super_names(
        super_names=super_names,
        form_type=form_type,
        min_similarity=0.9,
    )
    return {"pages": pages}


@router.get("/review-tab-customers")
async def get_review_tab_customers(
    year: Optional[int] = Query(None, description="ì—°ë„ (ì„ íƒ, ì—†ìœ¼ë©´ ì „ì²´)"),
    month: Optional[int] = Query(None, description="ì›” (ì„ íƒ, ì—†ìœ¼ë©´ ì „ì²´)"),
    db=Depends(get_db),
):
    """
    ê²€í†  íƒ­ì— ìˆëŠ” ëª¨ë“  ê±°ë˜ì²˜ ëª©ë¡ (ì •ë‹µì§€Â·ë²¡í„° ë“±ë¡ ë¬¸ì„œ ì œì™¸, itemsì˜ å¾—æ„å…ˆ/customer ì¤‘ë³µ ì œê±°).
    ê²€í†  íƒ­ì—ì„œ ê±°ë˜ì²˜ ëª©ë¡ ë²„íŠ¼ í´ë¦­ ì‹œ "ê²€í†  íƒ­ ì „ì²´ ê±°ë˜ì²˜" í‘œì‹œìš©.
    """
    in_vector = _get_in_vector_pdf_filenames(db)
    pdfs = db.get_review_tab_pdf_filenames(in_vector, year=year, month=month)
    customer_names = db.get_distinct_customer_names_for_pdfs(pdfs)
    return {"customer_names": customer_names}


@router.post("/pages-by-customers")
async def post_pages_by_customers(
    body: PagesByCustomersRequest,
    db=Depends(get_db),
    current_user=Depends(get_current_user),
):
    """
    ì„ íƒí•œ customerëª… ëª©ë¡ì— í•´ë‹¹í•˜ëŠ” í•­ëª©ì´ ìˆëŠ” í˜ì´ì§€ ëª©ë¡ ë°˜í™˜ (ì™„ì „ ì¼ì¹˜).
    ëª¨ë‹¬ì—ì„œ ç¢ºèª í›„ í•„í„° ì ìš©ìš©.
    """
    if not body.customer_names:
        return {"pages": []}
    pages = db.get_page_keys_by_customer_names(
        customer_names=body.customer_names,
        form_type=body.form_type,
    )
    return {"pages": pages}


@router.post("/customer-similarity-mapping")
async def post_customer_similarity_mapping(
    body: CustomerSimilarityMappingRequest,
    current_user=Depends(get_current_user),
):
    """
    ì™¼ìª½(ì‹¤ì œ ê±°ë˜ì²˜) ê°ê°ì— ëŒ€í•´ ì˜¤ë¥¸ìª½(ë‹´ë‹¹) ì¤‘ ìœ ì‚¬ë„ ìµœê³ ì¸ 1ê°œ + ì ìˆ˜ ë°˜í™˜.
    notepad.ipynb find_similar_supersì™€ ë™ì¼í•œ difflib ìœ ì‚¬ë„ ì‚¬ìš© (Levenshtein ì•„ë‹˜).
    """
    left_list = [s.strip() for s in (body.customer_names or []) if s is not None]
    right_list = [s.strip() for s in (body.super_names or []) if s is not None]
    used_rights = set()
    mapped: List[dict] = []
    for left in left_list:
        best_right = ""
        best_score = 0.0
        for right in right_list:
            score = _similarity_difflib(left, right)
            if score > best_score:
                best_score = score
                best_right = right
        if best_right:
            used_rights.add(best_right)
        mapped.append({"left": left, "right": best_right, "score": round(best_score, 4)})
    unmapped_rights = [r for r in right_list if r not in used_rights]
    return {"mapped": mapped, "unmapped_rights": unmapped_rights}


@router.get("/unit-price-by-product")
async def get_unit_price_by_product(
    product_name: str = Query(..., description="å•†å“åï¼ˆì œí’ˆëª…ï¼‰"),
    top_k: int = Query(5, ge=1, le=50, description="ë°˜í™˜ ê±´ìˆ˜ï¼ˆí‰ê·  ìœ ì‚¬ë„ ìˆœ ìƒìœ„ï¼‰"),
    min_similarity: float = Query(0.2, ge=0.0, le=1.0, description="ë¯¸ì‚¬ìš©ï¼ˆí˜¸í™˜ìš©ï¼‰"),
    sub_min_similarity: float = Query(0.0, ge=0.0, le=1.0, description="ë¯¸ì‚¬ìš©ï¼ˆí˜¸í™˜ìš©ï¼‰"),
):
    """
    ì œí’ˆëª…Â·ìš©ëŸ‰ ìœ ì‚¬ë„ë¥¼ ê°ê° ê³„ì‚°í•´ í‰ê· ìœ¼ë¡œ ì •ë ¬í•œ ìƒìœ„ top_k ë°˜í™˜. min_similarity í•„í„° ì—†ìŒ.
    """
    if not _UNIT_PRICE_CSV.exists():
        raise HTTPException(status_code=503, detail="unit_price.csv not found")
    try:
        base_name, capacity = split_name_and_capacity(product_name)
        sub_query = capacity if capacity else None
        df = find_similar_products(
            query=base_name,
            csv_path=_UNIT_PRICE_CSV,
            col="ì œí’ˆëª…",
            top_k=top_k,
            min_similarity=min_similarity,
            sub_col="ì œí’ˆìš©ëŸ‰",
            sub_query=sub_query,
            sub_min_similarity=sub_min_similarity,
        )
        # NaN ë“± ì²˜ë¦¬í•˜ì—¬ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        records = []
        for _, row in df.iterrows():
            rec = row.to_dict()
            for k, v in rec.items():
                if hasattr(v, "item") and callable(getattr(v, "item", None)):
                    try:
                        rec[k] = v.item()
                    except (ValueError, AttributeError):
                        rec[k] = None if pd.isna(v) else v
                elif hasattr(v, "__float__") and pd.isna(v):
                    rec[k] = None
                else:
                    rec[k] = v
            records.append(rec)
        return {
            "base_name": base_name,
            "capacity": capacity,
            "product_name_input": product_name,
            "matches": records,
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/product/candidates-by-sap-product")
async def get_product_candidates_by_sap_product(
    query: str = Query("", description="æ¤œç´¢èªï¼ˆì œí’ˆëª…ìœ¼ë¡œ ê²€ìƒ‰ï¼‰"),
    top_k: int = Query(10, ge=1, le=30, description="ë°˜í™˜ ê±´ìˆ˜"),
    min_similarity: float = Query(0.0, ge=0.0, le=1.0, description="ìµœì†Œ ìœ ì‚¬ë„"),
):
    """
    sap_product.csvì—ì„œ ì œí’ˆëª…ìœ¼ë¡œë§Œ ìœ ì‚¬ë„ ê²€ìƒ‰. ë‹¨ê°€ íƒ­ ìµœì¢…í›„ë³´ ê²€ìƒ‰ â†’ ì œí’ˆì½”ë“œë§Œ ë°˜í™˜.
    ë°˜í™˜: [{ ì œí’ˆì½”ë“œ, ì œí’ˆëª… }, ...] ï¼ˆä»•åˆ‡ãƒ»æœ¬éƒ¨é•·ëŠ” unit_priceì—ì„œ å•†å“ã‚³ãƒ¼ãƒ‰ë¡œ ë³„ë„ ì¡°íšŒï¼‰
    """
    q = (query or "").strip()
    if not q:
        return {"query": "", "matches": []}
    if not _SAP_PRODUCT_CSV.exists():
        return {"query": q, "matches": [], "skipped_reason": "sap_product.csv not found"}
    try:
        df_sap = pd.read_csv(_SAP_PRODUCT_CSV, dtype=str, encoding="utf-8-sig")
        df_sap.columns = [c.strip().lstrip("\ufeff") for c in df_sap.columns]
        scored = []
        for _, r in df_sap.iterrows():
            name = (r.get("ì œí’ˆëª…") or "").strip()
            sap_name = (r.get("SAPì œí’ˆëª…") or "").strip()
            code = (r.get("ì œí’ˆì½”ë“œ") or "").strip()
            if not code:
                continue
            s1 = _similarity_difflib(q, name) if name else 0.0
            s2 = _similarity_difflib(q, sap_name) if sap_name else 0.0
            score = max(s1, s2)
            if score >= min_similarity:
                scored.append((score, code, name or sap_name))
        scored.sort(key=lambda x: -x[0])
        rows = scored[:top_k]
        matches = [{"ì œí’ˆì½”ë“œ": code, "ì œí’ˆëª…": name or ""} for _, code, name in rows]
        return {"query": q, "matches": matches}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/unit-price-by-product-code")
async def get_unit_price_by_product_code(
    product_code: str = Query(..., description="å•†å“ã‚³ãƒ¼ãƒ‰ï¼ˆì œí’ˆì½”ë“œï¼‰"),
):
    """
    unit_price.csvì—ì„œ å•†å“ã‚³ãƒ¼ãƒ‰ë¡œ 1ê±´ ì¡°íšŒ. ä»•åˆ‡ãƒ»æœ¬éƒ¨é•· ìë™ì™„ì„±ìš©.
    ë°˜í™˜: { å•†å“ã‚³ãƒ¼ãƒ‰, ä»•åˆ‡, æœ¬éƒ¨é•· } or null
    """
    code = (product_code or "").strip()
    if not code:
        return {"row": None}
    if not _UNIT_PRICE_CSV.exists():
        return {"row": None}
    try:
        df = pd.read_csv(_UNIT_PRICE_CSV, dtype=str, encoding="utf-8-sig")
        df.columns = [c.strip().lstrip("\ufeff") for c in df.columns]
        for _, r in df.iterrows():
            c = (r.get("ì œí’ˆì½”ë“œ") or "").strip()
            if c != code:
                continue
            try:
                ì‹œí‚¤ë¦¬ = float(r.get("ì‹œí‚¤ë¦¬") or 0) if pd.notna(r.get("ì‹œí‚¤ë¦¬")) else None
                ë³¸ë¶€ì¥ = float(r.get("ë³¸ë¶€ì¥") or 0) if pd.notna(r.get("ë³¸ë¶€ì¥")) else None
            except (TypeError, ValueError):
                ì‹œí‚¤ë¦¬, ë³¸ë¶€ì¥ = None, None
            return {
                "row": {
                    "å•†å“ã‚³ãƒ¼ãƒ‰": code,
                    "ä»•åˆ‡": ì‹œí‚¤ë¦¬,
                    "æœ¬éƒ¨é•·": ë³¸ë¶€ì¥,
                },
            }
        return {"row": None}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/retail-candidates-by-customer")
async def get_retail_candidates_by_customer(
    customer_name: str = Query(..., description="ê±°ë˜ì²˜ëª…(å¾—æ„å…ˆ ë“±)"),
    top_k: int = Query(20, ge=1, le=100, description="ë°˜í™˜ ê±´ìˆ˜"),
    min_similarity: float = Query(0.5, ge=0.0, le=1.0, description="ì†Œë§¤ì²˜ëª… ìµœì†Œ ìœ ì‚¬ë„"),
):
    """
    ê±°ë˜ì²˜ëª…ìœ¼ë¡œ retail_user.csv ì†Œë§¤ì²˜ëª…ê³¼ ìœ ì‚¬ë„ ë§¤ì¹­ í›„ë³´ ë°˜í™˜.
    ì»¬ëŸ¼: ì†Œë§¤ì²˜ì½”ë“œ, ì†Œë§¤ì²˜ëª…, íŒë§¤ì²˜ì½”ë“œ, íŒë§¤ì²˜ëª… (íŒë§¤ì²˜ëŠ” dist_retail.csv).
    """
    if not _RETAIL_USER_CSV.exists():
        raise HTTPException(status_code=503, detail="retail_user.csv not found")
    customer_name = (customer_name or "").strip()
    if not customer_name:
        return {"customer_name_input": "", "matches": []}
    try:
        df_retail = pd.read_csv(_RETAIL_USER_CSV, dtype=str)
        # (ì†Œë§¤ì²˜ëª…, ì†Œë§¤ì²˜ì½”ë“œ) ìœ ì‚¬ë„ ê³„ì‚° â†’ ìƒìœ„ top_k, min_similarity ì´ìƒ
        scored = []
        for _, r in df_retail.iterrows():
            name = (r.get("ì†Œë§¤ì²˜ëª…") or "").strip()
            code = (r.get("ì†Œë§¤ì²˜ì½”ë“œ") or "").strip()
            if not name or not code:
                continue
            score = _similarity_difflib(customer_name, name)
            if score >= min_similarity:
                scored.append((score, name, code))
        scored.sort(key=lambda x: -x[0])
        retail_tuples = scored[:top_k]  # (score, ì†Œë§¤ì²˜ëª…, ì†Œë§¤ì²˜ì½”ë“œ)

        # dist_retail: ì†Œë§¤ì²˜ì½”ë“œ â†’ [(íŒë§¤ì²˜ì½”ë“œ, íŒë§¤ì²˜ëª…), ...] (ì²« í–‰ë§Œ ë˜ëŠ” ì „ë¶€)
        dist_by_retail = {}
        if _DIST_RETAIL_CSV.exists():
            df_dist = pd.read_csv(_DIST_RETAIL_CSV, dtype=str)
            for _, r in df_dist.iterrows():
                retail_code = (r.get("ì†Œë§¤ì²˜ì½”ë“œ") or "").strip()
                dist_code = (r.get("íŒë§¤ì²˜ì½”ë“œ") or "").strip()
                dist_name = (r.get("íŒë§¤ì²˜ëª…") or "").strip()
                if not retail_code:
                    continue
                if retail_code not in dist_by_retail:
                    dist_by_retail[retail_code] = []
                dist_by_retail[retail_code].append((dist_code, dist_name))

        matches = []
        for score, retail_name, retail_code in retail_tuples:
            dist_list = dist_by_retail.get(retail_code) or [(None, None)]
            for dist_code, dist_name in dist_list:
                matches.append({
                    "ì†Œë§¤ì²˜ì½”ë“œ": retail_code,
                    "ì†Œë§¤ì²˜ëª…": retail_name,
                    "íŒë§¤ì²˜ì½”ë“œ": dist_code or "",
                    "íŒë§¤ì²˜ëª…": dist_name or "",
                    "similarity": round(score, 4),
                })
        return {"customer_name_input": customer_name, "matches": matches}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


def _dist_by_retail_first() -> dict:
    """ì†Œë§¤ì²˜ì½”ë“œ â†’ (íŒë§¤ì²˜ì½”ë“œ, íŒë§¤ì²˜ëª…) ì²« í–‰ë§Œ. dist_retail ê¸°ì¤€."""
    out = {}
    if not _DIST_RETAIL_CSV.exists():
        return out
    try:
        df = pd.read_csv(_DIST_RETAIL_CSV, dtype=str)
        for _, r in df.iterrows():
            retail = (r.get("ì†Œë§¤ì²˜ì½”ë“œ") or "").strip()
            dist_c = (r.get("íŒë§¤ì²˜ì½”ë“œ") or "").strip()
            dist_n = (r.get("íŒë§¤ì²˜ëª…") or "").strip()
            if retail and retail not in out:
                out[retail] = (dist_c, dist_n)
    except Exception:
        pass
    return out


def _sap_dist_by_retail_first() -> dict:
    """ì†Œë§¤ì²˜ì½”ë“œ â†’ (íŒë§¤ì²˜ì½”ë“œ, íŒë§¤ì²˜ëª…) ì²« í–‰ë§Œ. sap_retail ê¸°ì¤€."""
    out = {}
    if not _SAP_RETAIL_CSV.exists():
        return out
    try:
        df = pd.read_csv(_SAP_RETAIL_CSV, dtype=str)
        for _, r in df.iterrows():
            retail = (r.get("ì†Œë§¤ì²˜ì½”ë“œ") or "").strip()
            dist_c = (r.get("íŒë§¤ì²˜ì½”ë“œ") or "").strip()
            dist_n = (r.get("íŒë§¤ì²˜ëª…") or "").strip()
            if retail and retail not in out:
                out[retail] = (dist_c, dist_n)
    except Exception:
        pass
    return out


def _dist_for_retail(retail_code: str) -> tuple:
    """ë§¤í•‘í•œ ì†Œë§¤ì²˜ì½”ë“œë¡œ íŒë§¤ì²˜ì½”ë“œ/íŒë§¤ì²˜ëª… ë°˜í™˜. sap_retail ìš°ì„ , ì—†ìœ¼ë©´ dist_retail."""
    code = (retail_code or "").strip()
    if not code:
        return ("", "")
    sap = _sap_dist_by_retail_first()
    if code in sap:
        return sap[code]
    return _dist_by_retail_first().get(code) or ("", "")


@router.get("/retail/by-customer-code")
async def get_retail_by_customer_code(
    customer_code: str = Query(..., description="å¾—æ„å…ˆCD(ë„ë§¤ì†Œë§¤ì²˜ì½”ë“œ)"),
):
    """
    å¾—æ„å…ˆCD(ë„ë§¤ì†Œë§¤ì²˜ì½”ë“œ)ë¡œ domae_retail_1 ì¡°íšŒ â†’ í•´ë‹¹ í–‰ì˜ ì†Œë§¤ì²˜ì½”ë“œ/ì†Œë§¤ì²˜ëª… 1ê±´ ë°˜í™˜.
    íŒë§¤ì²˜ì½”ë“œ/íŒë§¤ì²˜ëª…ì€ dist_retailì—ì„œ ì¡°íšŒí•´ í•¨ê»˜ ë°˜í™˜.
    """
    customer_code = (customer_code or "").strip()
    if not customer_code:
        return {"customer_code_input": "", "match": None, "skipped_reason": None}
    if not _DOMAE_RETAIL_1_CSV.exists():
        return {"customer_code_input": customer_code, "match": None, "skipped_reason": "domae_retail_1.csv not found"}
    try:
        df = pd.read_csv(_DOMAE_RETAIL_1_CSV, dtype=str)
        # å¾—æ„å…ˆCD = domae_retail_1ì˜ ë„ë§¤ì†Œë§¤ì²˜ì½”ë“œ(1ì—´). ì†Œë§¤ì²˜ì½”ë“œ(2ì—´)ë¡œ ì°©ê°í•˜ë©´ ì•ˆ ë¨
        row = df[df["ë„ë§¤ì†Œë§¤ì²˜ì½”ë“œ"].astype(str).str.strip() == customer_code]
        if row.empty:
            return {"customer_code_input": customer_code, "match": None, "skipped_reason": None}
        r = row.iloc[0]
        retail_code = (r.get("ì†Œë§¤ì²˜ì½”ë“œ") or "").strip()
        retail_name = (r.get("ì†Œë§¤ì²˜ëª…") or "").strip()
        dist_c, dist_n = _dist_for_retail(retail_code)
        return {
            "customer_code_input": customer_code,
            "match": {
                "ë„ë§¤ì†Œë§¤ì²˜ì½”ë“œ": customer_code,
                "ë„ë§¤ì†Œë§¤ì²˜ëª…": "",  # domae_retail_1ì—ëŠ” ë„ë§¤ì†Œë§¤ì²˜ëª… ì»¬ëŸ¼ ì—†ìŒ
                "ì†Œë§¤ì²˜ì½”ë“œ": retail_code,
                "ì†Œë§¤ì²˜ëª…": retail_name,
                "íŒë§¤ì²˜ì½”ë“œ": dist_c,
                "íŒë§¤ì²˜ëª…": dist_n,
                "similarity": 1.0,
            },
            "skipped_reason": None,
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/retail/candidates-by-shop-name")
async def get_retail_candidates_by_shop_name(
    customer_name: str = Query(..., description="ê±°ë˜ì²˜ëª…(å¾—æ„å…ˆ ë“±)"),
    top_k: int = Query(5, ge=1, le=20, description="ë°˜í™˜ ê±´ìˆ˜"),
    min_similarity: float = Query(0.0, ge=0.0, le=1.0, description="ì†Œë§¤ì²˜ëª… ìµœì†Œ ìœ ì‚¬ë„"),
):
    """
    ê±°ë˜ì²˜ëª…ìœ¼ë¡œ domae_retail_2ì˜ ì†Œë§¤ì²˜ëª…ê³¼ ìœ ì‚¬ë„ ë§¤ì¹­ í›„ë³´ ë°˜í™˜ (ì†Œë§¤ì²˜ì½”ë“œ + dist_retail íŒë§¤ì²˜).
    """
    customer_name = (customer_name or "").strip()
    if not customer_name:
        return {"customer_name_input": "", "matches": []}
    if not _DOMAE_RETAIL_2_CSV.exists():
        return {"customer_name_input": customer_name, "matches": [], "skipped_reason": "domae_retail_2.csv not found"}
    try:
        df = pd.read_csv(_DOMAE_RETAIL_2_CSV, dtype=str, encoding="utf-8-sig")
        df.columns = [c.strip().lstrip("\ufeff") for c in df.columns]
        cols = list(df.columns)
        scored = []
        for _, r in df.iterrows():
            # domae_retail_2 1ì—´ = ë„ë§¤ì†Œë§¤ì²˜ëª…. ì¸ë±ìŠ¤ë¡œ ì½ì–´ ì»¬ëŸ¼ëª…/ì¸ì½”ë”© ì´ìŠˆ íšŒí”¼
            domae_name = (str(r.iloc[0]) if len(cols) > 0 else "") or ""
            domae_name = (domae_name or "").strip()
            name = (r.get("ì†Œë§¤ì²˜ëª…") or "").strip()
            code = (r.get("ì†Œë§¤ì²˜ì½”ë“œ") or "").strip()
            retail_name = (r.get("ì†Œë§¤ì²˜ëª…") or "").strip()
            # ì¼ë¶€ í–‰ë§Œ 2ì—´/3ì—´ ê°’ì´ ë°˜ëŒ€ë¡œ ë“¤ì–´ê°„ ê²½ìš°: ì½”ë“œ ì»¬ëŸ¼ì— ì´ë¦„, ì´ë¦„ ì»¬ëŸ¼ì— ì½”ë“œ â†’ í‘œì‹œ ì‹œ ë³´ì •
            if code and retail_name:
                code_like = retail_name.isdigit() and len(retail_name) >= 4
                name_like = not code.isdigit() or "â– " in code
                if name_like and code_like:
                    code, retail_name = retail_name, code
            if not name or not code:
                continue
            score = _similarity_difflib(customer_name, name)
            if score >= min_similarity:
                scored.append((score, name, code, retail_name, domae_name))
        scored.sort(key=lambda x: -x[0])
        top = scored[:top_k]
        matches = []
        for score, shop_name, retail_code, retail_name, domae_name in top:
            dist_c, dist_n = _dist_for_retail(retail_code)  # ë³´ì •ëœ ì½”ë“œë¡œ íŒë§¤ì²˜ ì¡°íšŒ
            matches.append({
                "ë„ë§¤ì†Œë§¤ì²˜ì½”ë“œ": "",  # domae_retail_2ì—ëŠ” ë„ë§¤ì†Œë§¤ì²˜ì½”ë“œ ì»¬ëŸ¼ ì—†ìŒ
                "ë„ë§¤ì†Œë§¤ì²˜ëª…": domae_name,
                "ì†Œë§¤ì²˜ì½”ë“œ": retail_code,
                "ì†Œë§¤ì²˜ëª…": retail_name,
                "íŒë§¤ì²˜ì½”ë“œ": dist_c,
                "íŒë§¤ì²˜ëª…": dist_n,
                "similarity": round(score, 4),
            })
        return {"customer_name_input": customer_name, "matches": matches}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


def _retail_code_to_names() -> dict:
    """ì†Œë§¤ì²˜ì½”ë“œ â†’ (ì†Œë§¤ì²˜ëª…, íŒë§¤ì²˜ì½”ë“œ, íŒë§¤ì²˜ëª…). dist_retail ìš°ì„ , ì—†ìœ¼ë©´ retail_userë¡œ ì†Œë§¤ì²˜ëª…ë§Œ."""
    out = {}
    if _DIST_RETAIL_CSV.exists():
        try:
            df = pd.read_csv(_DIST_RETAIL_CSV, dtype=str)
            for _, r in df.iterrows():
                rc = (r.get("ì†Œë§¤ì²˜ì½”ë“œ") or "").strip()
                rn = (r.get("ì†Œë§¤ì²˜ëª…") or "").strip()
                dc = (r.get("íŒë§¤ì²˜ì½”ë“œ") or "").strip()
                dn = (r.get("íŒë§¤ì²˜ëª…") or "").strip()
                if rc and rc not in out:
                    out[rc] = (rn, dc, dn)
        except Exception:
            pass
    if _RETAIL_USER_CSV.exists():
        try:
            df = pd.read_csv(_RETAIL_USER_CSV, dtype=str)
            for _, r in df.iterrows():
                rc = (r.get("ì†Œë§¤ì²˜ì½”ë“œ") or "").strip()
                rn = (r.get("ì†Œë§¤ì²˜ëª…") or "").strip()
                if rc and rc not in out:
                    out[rc] = (rn, "", "")
                elif rc:
                    prev = out[rc]
                    if not prev[0] and rn:
                        out[rc] = (rn, prev[1], prev[2])
        except Exception:
            pass
    return out


@router.get("/retail/candidates-by-rag-answer")
async def get_retail_candidates_by_rag_answer(
    customer_name: str = Query(..., description="ê±°ë˜ì²˜ëª…(å¾—æ„å…ˆ)"),
    top_k: int = Query(5, ge=1, le=20, description="ë°˜í™˜ ê±´ìˆ˜"),
    min_similarity: float = Query(0.0, ge=0.0, le=1.0, description="ìµœì†Œ ìœ ì‚¬ë„"),
):
    """
    å¾—æ„å…ˆìœ¼ë¡œ íŒë§¤ì²˜-ì†Œë§¤ì²˜ RAG ì •ë‹µì§€ ë²¡í„° ì¸ë±ìŠ¤ ê²€ìƒ‰.
    ë°˜í™˜: RetailMatch í˜•ì‹(å¾—æ„å…ˆ, ì†Œë§¤ì²˜ì½”ë“œ, ì†Œë§¤ì²˜ëª…, íŒë§¤ì²˜ì½”ë“œ, íŒë§¤ì²˜ëª…, similarity).
    """
    customer_name = (customer_name or "").strip()
    if not customer_name:
        return {"customer_name_input": "", "matches": []}
    try:
        rag = get_rag_manager()
        raw = rag.search_retail_rag_answer(customer_name, top_k=top_k, min_similarity=min_similarity)
        if not raw:
            return {"customer_name_input": customer_name, "matches": [], "skipped_reason": "RAG ì •ë‹µì§€ ì¸ë±ìŠ¤ê°€ ë¹„ì–´ìˆê±°ë‚˜ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"}
        code_to_names = _retail_code_to_names()
        matches = []
        for r in raw:
            retail_code = (r.get("å°å£²å…ˆCD") or "").strip()
            dist_c = (r.get("å—æ³¨å…ˆCD") or "").strip()
            retail_n, dist_c_lookup, dist_n = code_to_names.get(retail_code, ("", "", ""))
            if not dist_c and dist_c_lookup:
                dist_c = dist_c_lookup
            if not dist_n and dist_c:
                dist_n = _dist_for_retail(retail_code)[1] or dist_n
            matches.append({
                "å¾—æ„å…ˆ": r.get("å¾—æ„å…ˆ", ""),
                "ë„ë§¤ì†Œë§¤ì²˜ì½”ë“œ": "",
                "ë„ë§¤ì†Œë§¤ì²˜ëª…": r.get("å¾—æ„å…ˆ", ""),
                "ì†Œë§¤ì²˜ì½”ë“œ": retail_code,
                "ì†Œë§¤ì²˜ëª…": retail_n or retail_code,
                "íŒë§¤ì²˜ì½”ë“œ": dist_c,
                "íŒë§¤ì²˜ëª…": dist_n or dist_c,
                "similarity": r.get("similarity", 0.0),
            })
        return {"customer_name_input": customer_name, "matches": matches}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/{pdf_filename}/pages/{page_number}/image")
async def get_page_image_url(
    pdf_filename: str,
    page_number: int,
    db=Depends(get_db)
):
    """
    í˜ì´ì§€ ì´ë¯¸ì§€ URL ì¡°íšŒ (page_role ì •ë³´ í¬í•¨)

    Args:
        pdf_filename: PDF íŒŒì¼ëª…
        page_number: í˜ì´ì§€ ë²ˆí˜¸
        db: ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
    """
    try:
        # ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ì¡°íšŒ
        image_path = db.get_page_image_path(pdf_filename, page_number)

        # page_role ì •ë³´ ì¡°íšŒ
        # documents.get_page_meta ì™€ ë™ì¼í•˜ê²Œ db.get_page_result ë¥¼ ì‚¬ìš©í•˜ì—¬
        # current / archive ë“± í…Œì´ë¸” êµ¬ì¡° ë³€ê²½ì— ìƒê´€ì—†ì´ ì¼ê´€ëœ ë°©ì‹ìœ¼ë¡œ page_role ì„ ê°€ì ¸ì˜¨ë‹¤.
        page_role = None
        try:
            page_result = db.get_page_result(pdf_filename, page_number)
            if page_result:
                page_role = page_result.get("page_role")
        except Exception:
            # page_role ì¡°íšŒ ì‹¤íŒ¨ ì‹œ None ìœ ì§€ (ë°°ì§€ ë¹„í‘œì‹œ)
            pass

        if not image_path:
            # ì´ë¯¸ì§€ê°€ ì•„ì§ DBì— ì—†ì–´ë„ 404 ëŒ€ì‹  200 + image_url: null ë°˜í™˜ (ê²€í†  íƒ­ ë“±ì—ì„œ ì—ëŸ¬ ëŒ€ì‹  ì•ˆë‚´ í‘œì‹œ)
            return {
                "image_url": None,
                "format": "jpeg",
                **({"page_role": page_role} if page_role else {}),
            }

        # Windows ë“±ì—ì„œ DBì— ë°±ìŠ¬ë˜ì‹œë¡œ ì €ì¥ëœ ê²½ë¡œë¥¼ URLìš© ìŠ¬ë˜ì‹œë¡œ ì •ê·œí™”
        image_path = image_path.replace("\\", "/")

        # íŒŒì¼ ì‹œìŠ¤í…œ ê²½ë¡œë¥¼ URL ê²½ë¡œë¡œ ë³€í™˜ ("static/images/..." -> "/static/images/...")
        if image_path.startswith("static/"):
            path_parts = image_path.split("/")
            encoded_parts = [quote(part, safe="") for part in path_parts]
            image_url = "/" + "/".join(encoded_parts)
        elif image_path.startswith("/"):
            path_parts = image_path[1:].split("/")
            encoded_parts = [quote(part, safe="") for part in path_parts]
            image_url = "/" + "/".join(encoded_parts)
        else:
            path_parts = image_path.split("/")
            encoded_parts = [quote(part, safe="") for part in path_parts]
            image_url = "/" + "/".join(encoded_parts)

        print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ URL ìƒì„±: {image_path} -> {image_url}")

        response = {
            "image_url": image_url,
            "format": "jpeg"
        }

        # page_roleì´ ìˆìœ¼ë©´ ì‘ë‹µì— í¬í•¨
        if page_role:
            response["page_role"] = page_role

        return response

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/{pdf_filename}/pages/{page_number}/ocr-text")
async def get_page_ocr_text(
    pdf_filename: str,
    page_number: int,
    db=Depends(get_db)
):
    """
    í˜ì´ì§€ OCR í…ìŠ¤íŠ¸ ì¡°íšŒ (ì •ë‹µì§€ ìƒì„± íƒ­ ì „ìš©).
    í•´ë‹µ ìƒì„± ë¸Œë¦¿ì§€ë¡œ ë„˜ì–´ì˜¨ ë¬¸ì„œëŠ” ì´ë¯¸ RAG íŒŒì‹±ìœ¼ë¡œ debug2ê°€ ìˆìœ¼ë¯€ë¡œ,
    ì™¸ë¶€ API(Azure) í˜¸ì¶œ ì—†ì´ DB ì €ì¥ë¶„ â†’ debug2ë§Œ ì‚¬ìš©.
    """
    try:
        pdf_name = pdf_filename
        if pdf_name.lower().endswith(".pdf"):
            pdf_name = pdf_name[:-4]

        ocr_text = ""

        # 0) DBì— ì €ì¥ëœ OCR (ì´ì „ ì €ì¥ ì‹œ í™”ë©´ì—ì„œ ë³´ë‚¸ ê°’)
        try:
            with db.get_connection() as conn:
                cur = conn.cursor()
                cur.execute(
                    "SELECT page_meta FROM page_data_current WHERE pdf_filename = %s AND page_number = %s",
                    (pdf_filename, page_number),
                )
                row = cur.fetchone()
                if row and row[0]:
                    meta = row[0] if isinstance(row[0], dict) else (json.loads(row[0]) if isinstance(row[0], str) else None)
                    if isinstance(meta, dict) and meta.get("_ocr_text"):
                        ocr_text = (meta["_ocr_text"] or "").strip()
        except Exception:
            pass

        # 1) debug2 (RAG íŒŒì‹± ì‹œ ì €ì¥ëœ íŒŒì¼) â€” ì™¸ë¶€ API í˜¸ì¶œ ì—†ìŒ
        if not ocr_text.strip():
            try:
                from modules.utils.config import get_project_root
                root = get_project_root()
                debug2_file = root / "debug2" / pdf_name / f"page_{page_number}_ocr_text.txt"
                if debug2_file.exists():
                    ocr_text = debug2_file.read_text(encoding="utf-8").strip()
            except Exception:
                pass

        return {"ocr_text": ocr_text or ""}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


class RerunOcrBody(BaseModel):
    """OCR ë‹¤ì‹œ ì¸ì‹ ìš”ì²­ (ì •ë‹µì§€ íƒ­: Azure ì „ìš©)"""
    provider: str = "azure"  # ì •ë‹µì§€ ì˜ì—­ì—ì„œëŠ” Azureë§Œ ì‚¬ìš©
    azure_model: Optional[str] = None  # prebuilt-read | prebuilt-layout | prebuilt-document (ê¸°ë³¸ prebuilt-layout)


@router.post("/{pdf_filename}/pages/{page_number}/ocr-rerun")
async def rerun_page_ocr(
    pdf_filename: str,
    page_number: int,
    body: RerunOcrBody,
    db=Depends(get_db),
):
    """
    í˜„ì¬ í˜ì´ì§€ì— ëŒ€í•´ Azure OCRì„ ë‹¤ì‹œ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ debug2ì— ì €ì¥í•œ ë’¤ ë°˜í™˜.
    ì •ë‹µì§€ ìƒì„± íƒ­ ì „ìš© â€” ì €ì¥ë˜ëŠ” OCR í…ìŠ¤íŠ¸ëŠ” í•­ìƒ Azure.
    """
    provider = (body.provider or "azure").strip().lower()
    if provider != "azure":
        raise HTTPException(status_code=400, detail="ì •ë‹µì§€ ì˜ì—­ì—ì„œëŠ” Azure OCRë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")

    pdf_name = pdf_filename if not pdf_filename.lower().endswith(".pdf") else pdf_filename[:-4]
    root = Path(__file__).resolve().parent.parent.parent.parent  # project root

    # 1) í˜ì´ì§€ ì´ë¯¸ì§€ ê²½ë¡œ (DB) ë˜ëŠ” PDF ê²½ë¡œ í™•ë³´
    image_path = db.get_page_image_path(pdf_filename, page_number)
    full_image_path = None
    if image_path:
        full_image_path = Path(image_path) if Path(image_path).is_absolute() else root / image_path
        if not full_image_path.exists():
            full_image_path = None

    pdf_path_str = None
    if not full_image_path:
        from modules.utils.pdf_utils import find_pdf_path
        pdf_path_str = find_pdf_path(pdf_name)
    pdf_path = Path(pdf_path_str) if pdf_path_str and Path(pdf_path_str).exists() else None

    if not full_image_path and not pdf_path:
        raise HTTPException(
            status_code=404,
            detail="í˜ì´ì§€ ì´ë¯¸ì§€ ë˜ëŠ” PDFë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ì €ì¥ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.",
        )

    ocr_text = ""

    if provider == "azure":
        try:
            from modules.core.extractors.azure_extractor import get_azure_extractor
            from modules.utils.table_ocr_utils import raw_to_full_text

            azure_model = (body.azure_model or "prebuilt-layout").strip() or "prebuilt-layout"
            extractor = get_azure_extractor(model_id=azure_model, enable_cache=False)
            raw = None
            if full_image_path:
                raw = await asyncio.to_thread(extractor.extract_from_image_raw, image_path=full_image_path)
            elif pdf_path:
                raw = await asyncio.to_thread(
                    extractor.extract_from_pdf_page_raw, pdf_path, page_number
                )
            if raw:
                ocr_text = raw_to_full_text(raw) or ""  # í‘œì‹œìš©: ì¸ì‹í•œ ì „ì²´ ë¬¸ìì—´
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Azure OCR ì‹¤íŒ¨: {e}")

    if not ocr_text.strip():
        raise HTTPException(
            status_code=422,
            detail="OCR ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ í’ˆì§ˆ ë˜ëŠ” í˜ì´ì§€ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
        )

    # debug2ì— ì €ì¥í•˜ì—¬ ì´í›„ get_page_ocr_textì—ì„œ ì´ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ë„ë¡ í•¨
    debug2_dir = root / "debug2" / pdf_name
    debug2_dir.mkdir(parents=True, exist_ok=True)
    ocr_file = debug2_dir / f"page_{page_number}_ocr_text.txt"
    ocr_file.write_text(ocr_text, encoding="utf-8")

    return {"ocr_text": ocr_text}
