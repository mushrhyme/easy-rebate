#!/usr/bin/env python3
"""
í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì—¬ëŸ¬ GPT / Gemini ëª¨ë¸ë¡œ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python compare_models.py <í…ìŠ¤íŠ¸_íŒŒì¼_ê²½ë¡œ>
    
ì˜ˆì‹œ:
    python compare_models.py debug2/æ—¥æœ¬ã‚¢ã‚¯ã‚»ã‚¹å’Œæ­Œå±±æ”¯åº—/page_2_prompt.txt

í•„ìš” í™˜ê²½ë³€ìˆ˜:
    - OPENAI_API_KEY: OpenAI API í‚¤
    - GEMINI_API_KEY: Google Gemini API í‚¤
    - ANTHROPIC_API_KEY: Anthropic Claude API í‚¤

ê²°ê³¼ë¬¼:
    - model_comparison_results/ í´ë”ì— ê° ëª¨ë¸ë³„ ê²°ê³¼ íŒŒì¼ ì €ì¥

í…ìŠ¤íŠ¸ ê¸¸ì´: 10,185 ë¬¸ì
ì…ë ¥ í† í° (GPT ê¸°ì¤€): 8,162 í† í°
âœ… 17,35s â†’ gpt_gpt-4o-2024-11-20
âœ… 45.11s â†’ gpt_gpt-5.2-2025-12-11
âœ… 23.04s â†’ claude_claude-haiku-4-5
âœ… 66.83s â†’ gemini_gemini-3-flash-preview
âœ… 64.66s â†’ gemini_gemini-2.5-pro
âœ… 28.32s â†’ gemini_gemini-2.5-flash
âœ… 28.55s â†’ gemini_gemini-2.0-flash
âœ… 14.22s â†’ gemini_gemini-2.5-flash-lites
âœ… 7.92s â†’ gemini_gemini-2.5-flash-lite-preview-09-2025
âœ… 26.08s â†’ gemini_gemini-2.0-flash-lite
"""

import json
import os
import re
import sys
import time
from pathlib import Path
from datetime import datetime

from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
PROJECT_ROOT = Path(__file__).resolve().parent
load_dotenv(PROJECT_ROOT / ".env")

# GPT ëª¨ë¸ ëª©ë¡
GPT_MODELS = [
    "gpt-4o-2024-11-20",
    # "gpt-5-mini-2025-08-07",
    # "gpt-5-nano-2025-08-07",
    "gpt-5.2-2025-12-11"

]

# Gemini ëª¨ë¸ ëª©ë¡ (í•„ìš”ì‹œ ìŠ¤í¬ë¦½íŠ¸ ìƒë‹¨ì—ì„œ ìˆ˜ì • ê°€ëŠ¥)
GEMINI_MODELS = [
    # "gemini-3-flash-preview",
    # "gemini-2.5-pro",
    # "gemini-2.5-flash",
    # "gemini-2.0-flash",
    "gemini-2.5-flash-lite",
    "gemini-2.5-flash-lite-preview-09-2025",
    # "gemini-2.0-flash-lite",
]

# Claude ëª¨ë¸ ëª©ë¡ (Haiku ë“±)
CLAUDE_MODELS = [
    "claude-haiku-4-5"
]


def call_gpt(model_name: str, text: str, api_key: str) -> tuple[str, float, str | None]:
    """GPT ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
    try:
        from openai import OpenAI

        client = OpenAI(api_key=api_key)
        start = time.time()
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": text}],
            timeout=60,
            max_completion_tokens=16000,
        )
        elapsed = time.time() - start
        result = response.choices[0].message.content or ""
        usage = None
        if hasattr(response, "usage") and response.usage:
            usage = f"prompt={response.usage.prompt_tokens}, completion={response.usage.completion_tokens}"
        return result, elapsed, usage
    except Exception as e:
        return f"[ERROR] {str(e)}", 0.0, None


def call_gemini(model_name: str, text: str, api_key: str) -> tuple[str, float, str | None]:
    """Gemini ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
    try:
        import google.generativeai as genai

        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(model_name)
        start = time.time()
        response = model.generate_content(text)
        elapsed = time.time() - start

        if not response.candidates or not response.candidates[0].content:
            return "[ERROR] Gemini ì‘ë‹µì— contentê°€ ì—†ìŠµë‹ˆë‹¤.", elapsed, None

        result_parts = []
        for part in response.candidates[0].content.parts:
            if hasattr(part, "text") and part.text:
                result_parts.append(part.text)
        result = "".join(result_parts) if result_parts else "[ERROR] ë¹ˆ ì‘ë‹µ"

        usage = None
        if hasattr(response, "usage_metadata") and response.usage_metadata:
            um = response.usage_metadata
            usage = f"prompt={getattr(um, 'prompt_token_count', 'N/A')}, completion={getattr(um, 'candidates_token_count', 'N/A')}"

        return result, elapsed, usage
    except Exception as e:
        return f"[ERROR] {str(e)}", 0.0, None


def call_claude(model_name: str, text: str, api_key: str) -> tuple[str, float, str | None]:
    """Claude ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
    try:
        from anthropic import Anthropic

        client = Anthropic(api_key=api_key)
        start = time.time()
        response = client.messages.create(
            model=model_name,
            max_tokens=16000,
            messages=[{"role": "user", "content": text}],
        )
        elapsed = time.time() - start

        result_parts = []
        for block in response.content:
            if hasattr(block, "text") and block.text:
                result_parts.append(block.text)
        result = "".join(result_parts) if result_parts else "[ERROR] ë¹ˆ ì‘ë‹µ"

        usage = None
        if hasattr(response, "usage") and response.usage:
            u = response.usage
            usage = f"prompt={getattr(u, 'input_tokens', 'N/A')}, completion={getattr(u, 'output_tokens', 'N/A')}"

        return result, elapsed, usage
    except Exception as e:
        return f"[ERROR] {str(e)}", 0.0, None


def count_input_tokens(text: str) -> int | None:
    """GPT(cl100k_base) ê¸°ì¤€ ì…ë ¥ í† í° ìˆ˜. tiktoken ë¯¸ì„¤ì¹˜ ì‹œ None."""
    try:
        import tiktoken
        enc = tiktoken.get_encoding("cl100k_base")
        return len(enc.encode(text))
    except Exception:
        return None


def sanitize_filename(name: str) -> str:
    """íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    return "".join(c if c.isalnum() or c in ".-_" else "_" for c in name)


def parse_json_from_response(result: str) -> dict | None:
    """
    LLM ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ JSONì„ ì¶”ì¶œí•˜ì—¬ íŒŒì‹±
    ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡(```json ... ```), Python None/True/False ë“±ì„ ì²˜ë¦¬
    """
    if not result or result.startswith("[ERROR]"):
        return None

    text = result.strip()

    # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
    if "```" in text:
        match = re.search(r"```(?:json)?\s*([\s\S]*?)```", text)
        if match:
            text = match.group(1).strip()
        else:
            # ``` ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°
            text = text.split("```", 1)[-1]
            if text.lower().startswith("json"):
                text = text[4:].strip()
            if "```" in text:
                text = text.rsplit("```", 1)[0].strip()

    # JSON ê°ì²´ ë˜ëŠ” ë°°ì—´ ì¶”ì¶œ
    obj_match = re.search(r"\{[\s\S]*\}", text)
    if obj_match:
        text = obj_match.group(0)

    # Python ë¬¸ë²•ì„ JSON í‘œì¤€ìœ¼ë¡œ ì¹˜í™˜
    text = re.sub(r":\s*None\s*([,}])", r": null\1", text)
    text = re.sub(r":\s*True\s*([,}])", r": true\1", text)
    text = re.sub(r":\s*False\s*([,}])", r": false\1", text)
    text = re.sub(r":\s*NaN\s*([,}])", r": null\1", text, flags=re.IGNORECASE)

    try:
        parsed = json.loads(text)
        if isinstance(parsed, list):
            return {"items": parsed, "page_role": "detail"}
        return parsed if isinstance(parsed, dict) else None
    except json.JSONDecodeError:
        return None


def run_comparison(text_path: str) -> None:
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    path = Path(text_path)
    if not path.is_absolute():
        path = PROJECT_ROOT / text_path

    if not path.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
        sys.exit(1)

    with open(path, "r", encoding="utf-8") as f:
        text = f.read()

    print(f"ğŸ“„ ì…ë ¥ íŒŒì¼: {path}")
    print(f"   í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text):,} ë¬¸ì")
    n_tokens = count_input_tokens(text)
    if n_tokens is not None:
        print(f"   ì…ë ¥ í† í° (GPT ê¸°ì¤€): {n_tokens:,} í† í°")
    else:
        print(f"   ì…ë ¥ í† í°: (tiktoken ë¯¸ì„¤ì¹˜ë¡œ ìƒëµ, pip install tiktoken)")
    print()

    output_dir = PROJECT_ROOT / "model_comparison_results"
    output_dir.mkdir(exist_ok=True)

    base_name = sanitize_filename(path.stem)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = output_dir / f"{base_name}_{timestamp}"
    run_dir.mkdir(exist_ok=True)

    # ì…ë ¥ í…ìŠ¤íŠ¸ ë³µì‚¬ (ì°¸ê³ ìš©)
    input_copy = run_dir / "input_text.txt"
    with open(input_copy, "w", encoding="utf-8") as f:
        f.write(text)
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {run_dir}\n")

    openai_key = os.getenv("OPENAI_API_KEY")
    gemini_key = os.getenv("GEMINI_API_KEY")
    anthropic_key = os.getenv("ANTHROPIC_API_KEY")

    if not openai_key:
        print("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GPT ëª¨ë¸ì€ ê±´ë„ˆëœë‹ˆë‹¤.")
    if not gemini_key:
        print("âš ï¸ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Gemini ëª¨ë¸ì€ ê±´ë„ˆëœë‹ˆë‹¤.")
    if not anthropic_key:
        print("âš ï¸ ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Claude ëª¨ë¸ì€ ê±´ë„ˆëœë‹ˆë‹¤.")

    all_results = []

    # GPT ëª¨ë¸ ì‹¤í–‰
    for model in GPT_MODELS:
        if not openai_key:
            continue
        print(f"ğŸ”„ GPT {model} ì‹¤í–‰ ì¤‘...", end=" ", flush=True)
        result, elapsed, usage = call_gpt(model, text, openai_key)
        safe_name = sanitize_filename(model)
        out_file = run_dir / f"gpt_{safe_name}.txt"
        header = f"# Model: {model}\n# Elapsed: {elapsed:.2f}s\n"
        if usage:
            header += f"# Usage: {usage}\n"
        header += "# ---\n\n"
        with open(out_file, "w", encoding="utf-8") as f:
            f.write(header + result)
        status = "âœ…" if not result.startswith("[ERROR]") else "âŒ"
        parsed = parse_json_from_response(result)
        if parsed is not None:
            json_file = run_dir / f"gpt_{safe_name}.json"
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(parsed, f, ensure_ascii=False, indent=2)
            print(f"{status} {elapsed:.2f}s â†’ {out_file.name}, {json_file.name}")
        else:
            print(f"{status} {elapsed:.2f}s â†’ {out_file.name}")
        all_results.append(("GPT", model, elapsed, result.startswith("[ERROR]")))

    # Claude ëª¨ë¸ ì‹¤í–‰
    for model in CLAUDE_MODELS:
        if not anthropic_key:
            continue
        print(f"ğŸ”„ Claude {model} ì‹¤í–‰ ì¤‘...", end=" ", flush=True)
        result, elapsed, usage = call_claude(model, text, anthropic_key)
        safe_name = sanitize_filename(model)
        out_file = run_dir / f"claude_{safe_name}.txt"
        header = f"# Model: {model}\n# Elapsed: {elapsed:.2f}s\n"
        if usage:
            header += f"# Usage: {usage}\n"
        header += "# ---\n\n"
        with open(out_file, "w", encoding="utf-8") as f:
            f.write(header + result)
        status = "âœ…" if not result.startswith("[ERROR]") else "âŒ"
        parsed = parse_json_from_response(result)
        if parsed is not None:
            json_file = run_dir / f"claude_{safe_name}.json"
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(parsed, f, ensure_ascii=False, indent=2)
            print(f"{status} {elapsed:.2f}s â†’ {out_file.name}, {json_file.name}")
        else:
            print(f"{status} {elapsed:.2f}s â†’ {out_file.name}")
        all_results.append(("Claude", model, elapsed, result.startswith("[ERROR]")))

    # Gemini ëª¨ë¸ ì‹¤í–‰
    for model in GEMINI_MODELS:
        if not gemini_key:
            continue
        print(f"ğŸ”„ Gemini {model} ì‹¤í–‰ ì¤‘...", end=" ", flush=True)
        result, elapsed, usage = call_gemini(model, text, gemini_key)
        safe_name = sanitize_filename(model)
        out_file = run_dir / f"gemini_{safe_name}.txt"
        header = f"# Model: {model}\n# Elapsed: {elapsed:.2f}s\n"
        if usage:
            header += f"# Usage: {usage}\n"
        header += "# ---\n\n"
        with open(out_file, "w", encoding="utf-8") as f:
            f.write(header + result)
        status = "âœ…" if not result.startswith("[ERROR]") else "âŒ"
        parsed = parse_json_from_response(result)
        if parsed is not None:
            json_file = run_dir / f"gemini_{safe_name}.json"
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(parsed, f, ensure_ascii=False, indent=2)
            print(f"{status} {elapsed:.2f}s â†’ {out_file.name}, {json_file.name}")
        else:
            print(f"{status} {elapsed:.2f}s â†’ {out_file.name}")
        all_results.append(("Gemini", model, elapsed, result.startswith("[ERROR]")))

    # ìš”ì•½
    print("\n" + "=" * 50)
    print("ğŸ“Š ìš”ì•½")
    print("=" * 50)
    for provider, model, elapsed, failed in all_results:
        status = "ì‹¤íŒ¨" if failed else f"{elapsed:.2f}s"
        print(f"  {provider} {model}: {status}")
    print(f"\nê²°ê³¼ í´ë”: {run_dir}")


def main():
    if len(sys.argv) < 2:
        print(__doc__)
        print("\nì‚¬ìš©ë²•: python compare_models.py <í…ìŠ¤íŠ¸_íŒŒì¼_ê²½ë¡œ>")
        print("\nì˜ˆì‹œ:")
        print('  python compare_models.py "debug2/æ—¥æœ¬ã‚¢ã‚¯ã‚»ã‚¹å’Œæ­Œå±±æ”¯åº—/page_2_prompt.txt"')
        sys.exit(1)

    text_path = sys.argv[1]
    run_comparison(text_path)


if __name__ == "__main__":
    main()
