# 동시 분석 시 성능 차이 원인

## 현상

- **한 탭에서 문서 A 분석 → 끝난 뒤 문서 B 분석**: 정상 체감 성능
- **탭 1에서 문서 A 분석 중, 탭 2에서 문서 B 분석 요청**: 둘 다 느려지거나 LLM 결과가 불안정해지는 느낌

## 원인

두 요청이 **같은 프로세스 안에서 동시에** 돌아가며, **공유 자원**을 같이 쓰기 때문입니다.

### 1. RAG 검색 공유 (가장 유력)

- **전역 싱글톤**: `get_rag_manager()` → 하나의 `RAGManager` 인스턴스만 사용
- **같은 임베딩 모델**: `SentenceTransformer` 한 개. 두 분석이 동시에 `model.encode()` 호출
  - CPU/메모리 경합, PyTorch 내부 동작으로 인해 지연·불안정 가능
- **같은 BM25/FAISS 인덱스**: `_build_bm25_index()`가 동시에 호출되면 race 가능, 검색 시에도 공유 구조체 동시 접근
- **결과**: RAG로 “유사 예제”를 찾는 단계가 두 요청에서 겹치면, 검색 지연·변동이 커지고, 그 다음 LLM 단계에도 영향을 줄 수 있음

### 2. API·리소스 경합

- **OpenAI**: 두 분석이 동시에 페이지 수 × 1회씩 LLM 호출 → RPM/TPM 제한에 걸리거나 지연 증가 가능
- **스레드 풀**: `run_in_executor(None, ...)`로 기본 스레드 풀 사용. 한 분석이 `extract_pages_with_rag` 내부에서 또 스레드 풀(예: 5 워커)을 쓰므로, 동시 두 분석이면 스레드 수가 크게 늘어나 CPU 경합이 커짐

### 3. 정리

| 구분 | 한 번에 한 문서만 분석 | 두 탭에서 동시 분석 |
|------|------------------------|---------------------|
| RAG 검색 | 한 스레드만 임베딩/BM25/FAISS 사용 | 두 스레드가 같은 RAGManager·같은 모델 동시 사용 → 경합 |
| LLM 호출 | 한 문서의 페이지만 동시 호출 | 두 문서의 페이지들이 한꺼번에 API 호출 → rate limit·지연 가능 |
| CPU | 한 작업 부하만 | 두 작업 부하 겹침 |

그래서 **동시에 다른 탭에서 분석할 때**가 상대적으로 느리거나 불안정하게 느껴지는 것이 자연스러운 동작에 가깝습니다.

## 대응 (구현된 것)

- **RAG 검색 직렬화**: `RAGManager`에 `_search_lock`을 두고, `search_similar_advanced()` 진입 시 이 락을 잡도록 함.
  - 효과: 서로 다른 탭의 분석이라도 **RAG 검색(임베딩 + BM25 + FAISS)은 한 번에 하나만** 실행됨.
  - 한쪽이 검색하는 동안 다른 쪽은 검색만 잠시 대기하고, LLM 호출은 각자 스레드 풀에서 그대로 병렬로 수행됨.
- 기대: 동시 분석 시에도 RAG 단계에서의 경합이 줄어들어, **체감 성능과 안정성이 나아질 수 있음**. (완전히 “한 문서씩만” 할 때와 동일해지지는 않음.)

## 추가로 할 수 있는 것

- **동시 분석 개수 제한**: 예를 들어 “분석 중인 문서가 이미 1개 있으면 새 분석은 대기” 같은 세마포어를 두면, 리소스 경합을 더 줄일 수 있음. (구현 시 FastAPI/백엔드에서 동시 실행 수를 제한하는 방식으로 가능.)
- **OpenAI rate limit 대응**: 동시 요청 수를 제한하거나, 429 시 백오프·재시도 정책을 두면 동시 분석 시 LLM 단계도 더 안정화할 수 있음.
