"""
RAG (Retrieval-Augmented Generation) ê´€ë¦¬ ëª¨ë“ˆ

FAISSë¥¼ ì‚¬ìš©í•˜ì—¬ OCR í…ìŠ¤íŠ¸ì™€ ì •ë‹µ JSON ìŒì„ ì €ì¥í•˜ê³  ê²€ìƒ‰í•©ë‹ˆë‹¤.
"""

import os
import json
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
from threading import Lock
import faiss


class RAGManager:
    """
    RAG ë²¡í„° DB ê´€ë¦¬ í´ë˜ìŠ¤
    
    FAISSë¥¼ ì‚¬ìš©í•˜ì—¬ OCR í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•˜ê³  ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    
    _model_lock = Lock()
    
    def __init__(self, persist_directory: Optional[str] = None, use_db: bool = True):
        self.use_db = use_db
        
        if persist_directory is None:
            from modules.utils.config import get_project_root
            project_root = get_project_root()
            persist_directory = str(project_root / "rag_db")
        
        self.persist_directory = persist_directory
        
        if self.use_db:
            from database.registry import get_db
            import psycopg2
            self.db = get_db()
            self._ensure_vector_index_table_exists()
        else:
            os.makedirs(persist_directory, exist_ok=True, mode=0o755)
            self.base_index_path = os.path.join(persist_directory, "base.faiss")
            self.base_metadata_path = os.path.join(persist_directory, "base_metadata.json")
        
        self._embedding_model = None
        self.index = None
        self.metadata = {}
        self.id_to_index = {}
        self.index_to_id = {}
        self._load_index()
        self._bm25_index = None
        self._bm25_texts = None
        self._bm25_example_map = None
    
    def _get_embedding_model(self):
        if self._embedding_model is None:
            with RAGManager._model_lock:
                if self._embedding_model is None:
                    try:
                        os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
                        from sentence_transformers import SentenceTransformer
                        self._embedding_model = SentenceTransformer(
                            'paraphrase-multilingual-MiniLM-L12-v2'
                        )
                    except ImportError:
                        raise ImportError(
                            "sentence-transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
                            "ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install sentence-transformers"
                        )
        return self._embedding_model
    
    def _get_embedding_dim(self) -> int:
        model = self._get_embedding_model()
        test_embedding = model.encode(["test"], convert_to_numpy=True)
        return test_embedding.shape[1]
    
    def _load_index(self):
        embedding_dim = self._get_embedding_dim()
        
        if self.use_db:
            self.index, self.metadata, self.id_to_index, self.index_to_id = self._load_index_from_db()
            if self.index is None:
                self.index = faiss.IndexFlatL2(embedding_dim)
                self.metadata = {}
                self.id_to_index = {}
                self.index_to_id = {}
        else:
            if os.path.exists(self.base_index_path):
                try:
                    self.index = faiss.read_index(self.base_index_path)
                except Exception as e:
                    print(f"âš ï¸ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±: {e}")
                    self.index = faiss.IndexFlatL2(embedding_dim)
            else:
                self.index = faiss.IndexFlatL2(embedding_dim)
            self.metadata, self.id_to_index, self.index_to_id = self._load_metadata_from_file(self.base_metadata_path)
        
        if len(self.index_to_id) < len(self.id_to_index):
            self.index_to_id = {idx: doc_id for doc_id, idx in self.id_to_index.items()}
            self._save_index()
    
    def reload_index(self):
        self._load_index()
        self._refresh_bm25_index()
        self.count_examples()
    
    def _ensure_vector_index_table_exists(self):
        try:
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_name = 'rag_vector_index'
                    )
                """)
                table_exists = cursor.fetchone()[0]
                
                if not table_exists:
                    cursor.execute("""
                        CREATE TABLE rag_vector_index (
                            index_id SERIAL PRIMARY KEY,
                            index_name VARCHAR(100) NOT NULL DEFAULT 'base',
                            form_type VARCHAR(10) NOT NULL,
                            index_data BYTEA NOT NULL,
                            metadata_json JSONB NOT NULL,
                            index_size BIGINT,
                            vector_count INTEGER DEFAULT 0,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            UNIQUE(index_name, form_type)
                        )
                    """)
                    cursor.execute("""
                        CREATE INDEX idx_rag_vector_index_name 
                        ON rag_vector_index(index_name)
                    """)
                    cursor.execute("""
                        CREATE INDEX idx_rag_vector_index_form_type 
                        ON rag_vector_index(form_type)
                    """)
                    cursor.execute("""
                        CREATE INDEX idx_rag_vector_index_name_form 
                        ON rag_vector_index(index_name, form_type)
                    """)
                else:
                    cursor.execute("""
                        SELECT EXISTS (
                            SELECT FROM information_schema.columns 
                            WHERE table_name = 'rag_vector_index' 
                            AND column_name = 'form_type'
                        )
                    """)
                    has_form_type = cursor.fetchone()[0]
                    if not has_form_type:
                        cursor.execute("""
                            ALTER TABLE rag_vector_index 
                            ADD COLUMN form_type VARCHAR(10)
                        """)
                        cursor.execute("""
                            ALTER TABLE rag_vector_index 
                            DROP CONSTRAINT IF EXISTS rag_vector_index_index_name_key
                        """)
                        cursor.execute("""
                            ALTER TABLE rag_vector_index 
                            ADD CONSTRAINT rag_vector_index_index_name_form_type_key 
                            UNIQUE (index_name, form_type)
                        """)
                        cursor.execute("""
                            CREATE INDEX IF NOT EXISTS idx_rag_vector_index_form_type 
                            ON rag_vector_index(form_type)
                        """)
                        cursor.execute("""
                            CREATE INDEX IF NOT EXISTS idx_rag_vector_index_name_form 
                            ON rag_vector_index(index_name, form_type)
                        """)
        except Exception as e:
            print(f"âš ï¸ í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def _load_index_from_db(self, form_type: Optional[str] = None) -> Tuple[Optional[Any], Dict[str, Any], Dict[str, int], Dict[int, str]]:
        try:
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                
                # ë””ë²„ê¹…: DBì— ìˆëŠ” ì¸ë±ìŠ¤ ëª©ë¡ í™•ì¸
                cursor.execute("""
                    SELECT index_name, form_type, vector_count, updated_at
                    FROM rag_vector_index
                    ORDER BY updated_at DESC
                """)
                all_indices = cursor.fetchall()                
                if form_type:
                    base_index_name = f'base_{form_type}'
                    # print(f"ğŸ” [ì¸ë±ìŠ¤ ë¡œë“œ] ì–‘ì‹ì§€ {form_type}ì˜ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„: {base_index_name}")
                    cursor.execute("""
                        SELECT index_data, metadata_json, vector_count
                        FROM rag_vector_index
                        WHERE index_name = %s AND form_type = %s
                        ORDER BY updated_at DESC
                        LIMIT 1
                    """, (base_index_name, form_type))
                else:
                    cursor.execute("""
                        SELECT index_data, metadata_json, vector_count
                        FROM rag_vector_index
                        WHERE index_name = 'base' AND (form_type IS NULL OR form_type = '')
                        ORDER BY updated_at DESC
                        LIMIT 1
                    """)
                
                row = cursor.fetchone()
                if row and len(row) >= 3:
                    index_data_bytes = row[0]
                    metadata_json = row[1]
                    vector_count = row[2] or 0
                    # print(f"âœ… [ì¸ë±ìŠ¤ ë¡œë“œ] base ì¸ë±ìŠ¤ ë°œê²¬: ë²¡í„° {vector_count}ê°œ")
                    if isinstance(index_data_bytes, memoryview):
                        index_data_bytes = np.frombuffer(index_data_bytes, dtype=np.uint8)
                    elif isinstance(index_data_bytes, bytes):
                        index_data_bytes = np.frombuffer(index_data_bytes, dtype=np.uint8)
                    else:
                        index_data_bytes = np.frombuffer(bytes(index_data_bytes), dtype=np.uint8)
                    
                    index = faiss.deserialize_index(index_data_bytes)
                    metadata = metadata_json.get('metadata', {})
                    id_to_index = metadata_json.get('id_to_index', {})
                    index_to_id_raw = metadata_json.get('index_to_id', {})
                    index_to_id = {int(k): v for k, v in index_to_id_raw.items()}
                    # print(f"âœ… [ì¸ë±ìŠ¤ ë¡œë“œ] ë¡œë“œ ì™„ë£Œ: ì¸ë±ìŠ¤ ntotal={index.ntotal}, ë©”íƒ€ë°ì´í„°={len(metadata)}ê°œ")
                    return index, metadata, id_to_index, index_to_id
                else:
                    if form_type:
                        print(f"âš ï¸ [ì¸ë±ìŠ¤ ë¡œë“œ] base_{form_type} ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. shard ê²€ìƒ‰ ì‹œë„...")
                
                if form_type:
                    if isinstance(form_type, (tuple, list)):
                        form_type = form_type[0] if form_type else None
                    
                    if not isinstance(form_type, str):
                        return None, {}, {}, {}
                    
                    cursor.execute("""
                        SELECT index_data, metadata_json, vector_count, index_name
                        FROM rag_vector_index
                        WHERE index_name LIKE 'shard_%' AND form_type = %s
                        ORDER BY updated_at DESC
                    """, (form_type,))
                else:
                    cursor.execute("""
                        SELECT index_data, metadata_json, vector_count, index_name
                        FROM rag_vector_index
                        WHERE index_name LIKE 'shard_%'
                        ORDER BY updated_at DESC
                    """)
                
                shard_rows = cursor.fetchall()
                if not shard_rows:
                    if form_type:
                        print(f"âš ï¸ [ì¸ë±ìŠ¤ ë¡œë“œ] ì–‘ì‹ì§€ {form_type}ì˜ shard ì¸ë±ìŠ¤ë„ ì—†ìŠµë‹ˆë‹¤.")
                    return None, {}, {}, {}
                embedding_dim = self._get_embedding_dim()
                if len(shard_rows) == 0:
                    return None, {}, {}, {}
                if len(shard_rows[0]) < 4:
                    return None, {}, {}, {}
                
                first_shard_data, first_metadata_json, first_vector_count, first_shard_name = shard_rows[0]
                if isinstance(first_shard_data, memoryview):
                    first_shard_data = np.frombuffer(first_shard_data, dtype=np.uint8)
                elif isinstance(first_shard_data, bytes):
                    first_shard_data = np.frombuffer(first_shard_data, dtype=np.uint8)
                else:
                    first_shard_data = np.frombuffer(bytes(first_shard_data), dtype=np.uint8)
                
                base_index = faiss.deserialize_index(first_shard_data)
                base_metadata = first_metadata_json.get('metadata', {})
                base_id_to_index = first_metadata_json.get('id_to_index', {})
                base_index_to_id_raw = first_metadata_json.get('index_to_id', {})
                base_index_to_id = {int(k): v for k, v in base_index_to_id_raw.items()}
                
                for shard_row in shard_rows[1:]:
                    if len(shard_row) < 4:
                        continue
                    shard_data_bytes, shard_metadata_json, shard_vector_count, shard_name = shard_row
                    if isinstance(shard_data_bytes, memoryview):
                        shard_data_bytes = np.frombuffer(shard_data_bytes, dtype=np.uint8)
                    elif isinstance(shard_data_bytes, bytes):
                        shard_data_bytes = np.frombuffer(shard_data_bytes, dtype=np.uint8)
                    else:
                        shard_data_bytes = np.frombuffer(bytes(shard_data_bytes), dtype=np.uint8)
                    
                    shard_index = faiss.deserialize_index(shard_data_bytes)
                    base_vector_count = base_index.ntotal
                    base_index.merge_from(shard_index)
                    shard_metadata = shard_metadata_json.get('metadata', {})
                    shard_id_to_index = shard_metadata_json.get('id_to_index', {})
                    shard_index_to_id_raw = shard_metadata_json.get('index_to_id', {})
                    shard_index_to_id = {int(k): v for k, v in shard_index_to_id_raw.items()}
                    for doc_id, shard_faiss_idx in shard_id_to_index.items():
                        new_faiss_idx = base_vector_count + shard_faiss_idx
                        base_metadata[doc_id] = shard_metadata.get(doc_id, {})
                        base_id_to_index[doc_id] = new_faiss_idx
                        base_index_to_id[new_faiss_idx] = doc_id
                
                total_vectors = base_index.ntotal
                # print(f"âœ… [ì¸ë±ìŠ¤ ë¡œë“œ] shard ë³‘í•© ì™„ë£Œ: ì´ {total_vectors}ê°œ ë²¡í„°, ë©”íƒ€ë°ì´í„° {len(base_metadata)}ê°œ")
                if form_type:
                    try:
                        self._save_merged_index_to_db(base_index, base_metadata, base_id_to_index, base_index_to_id, total_vectors, form_type)
                        print(f"âœ… [ì¸ë±ìŠ¤ ë¡œë“œ] base_{form_type} ì¸ë±ìŠ¤ë¡œ ì €ì¥ ì™„ë£Œ")
                    except Exception as save_err:
                        print(f"âš ï¸ base ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì‚¬ìš© ê°€ëŠ¥): {save_err}")
                
                return base_index, base_metadata, base_id_to_index, base_index_to_id
                
        except Exception as e:
            print(f"âš ï¸ DBì—ì„œ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None, {}, {}, {}
    
    def _load_metadata_from_file(self, metadata_path: str) -> Tuple[Dict[str, Any], Dict[str, int], Dict[int, str]]:
        if not os.path.exists(metadata_path):
            return {}, {}, {}
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                metadata = data.get("metadata", {})
                id_to_index = data.get("id_to_index", {})
                index_to_id_raw = data.get("index_to_id", {})
                index_to_id = {int(k): v for k, v in index_to_id_raw.items()}
                return metadata, id_to_index, index_to_id
        except Exception as e:
            print(f"âš ï¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}, {}, {}
    
    def _save_index(self):
        try:
            if self.use_db:
                self._save_index_to_db()
            else:
                faiss.write_index(self.index, self.base_index_path)
                data = {
                    "metadata": self.metadata,
                    "id_to_index": self.id_to_index,
                    "index_to_id": self.index_to_id
                }
                with open(self.base_metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _save_merged_index_to_db(
        self, 
        index: Any, 
        metadata: Dict[str, Any], 
        id_to_index: Dict[str, int], 
        index_to_id: Dict[int, str],
        vector_count: int,
        form_type: str
    ):
        try:
            serialized = faiss.serialize_index(index)
            if hasattr(serialized, 'tobytes'):
                index_data_bytes = serialized.tobytes()
            else:
                index_data_bytes = bytes(serialized)
            index_size = len(index_data_bytes)
            def clean_for_json(obj):
                import math
                if isinstance(obj, dict):
                    return {k: clean_for_json(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [clean_for_json(item) for item in obj]
                elif isinstance(obj, float):
                    if math.isnan(obj) or math.isinf(obj):
                        return None
                    return obj
                return obj
            cleaned_metadata = clean_for_json(metadata)
            metadata_json = {
                "metadata": cleaned_metadata,
                "id_to_index": id_to_index,
                "index_to_id": {str(k): v for k, v in index_to_id.items()}
            }
            base_index_name = f'base_{form_type}'
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT INTO rag_vector_index (
                        index_name, form_type, index_data, metadata_json, index_size, vector_count
                    ) VALUES (%s, %s, %s, %s::jsonb, %s, %s)
                    ON CONFLICT (index_name, form_type)
                    DO UPDATE SET
                        index_data = EXCLUDED.index_data,
                        metadata_json = EXCLUDED.metadata_json,
                        index_size = EXCLUDED.index_size,
                        vector_count = EXCLUDED.vector_count,
                        updated_at = CURRENT_TIMESTAMP
                """, (
                    base_index_name,
                    form_type,
                    index_data_bytes,
                    json.dumps(metadata_json, allow_nan=False),
                    index_size,
                    vector_count
                ))
        except Exception as e:
            print(f"âš ï¸ ë³‘í•© ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def _save_index_to_db(self):
        try:
            serialized = faiss.serialize_index(self.index)
            if hasattr(serialized, 'tobytes'):
                index_data_bytes = serialized.tobytes()
            else:
                index_data_bytes = bytes(serialized)
            index_size = len(index_data_bytes)
            vector_count = self.index.ntotal if self.index else 0
            def clean_for_json(obj):
                import math
                if isinstance(obj, dict):
                    return {k: clean_for_json(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [clean_for_json(item) for item in obj]
                elif isinstance(obj, float):
                    if math.isnan(obj) or math.isinf(obj):
                        return None
                    return obj
                return obj
            cleaned_metadata = clean_for_json(self.metadata)
            form_type_groups = {}
            for doc_id, data in cleaned_metadata.items():
                metadata_info = data.get("metadata", {})
                form_type = metadata_info.get("form_type") if isinstance(metadata_info, dict) else None
                if not form_type:
                    continue
                if form_type not in form_type_groups:
                    form_type_groups[form_type] = {
                        "metadata": {},
                        "id_to_index": {},
                        "index_to_id": {}
                    }
                form_type_groups[form_type]["metadata"][doc_id] = data
                if doc_id in self.id_to_index:
                    form_type_groups[form_type]["id_to_index"][doc_id] = self.id_to_index[doc_id]
                if doc_id in self.index_to_id:
                    faiss_idx = self.id_to_index.get(doc_id)
                    if faiss_idx is not None and faiss_idx in self.index_to_id:
                        form_type_groups[form_type]["index_to_id"][str(faiss_idx)] = self.index_to_id[faiss_idx]
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                for form_type, group_data in form_type_groups.items():
                    metadata_json = {
                        "metadata": group_data["metadata"],
                        "id_to_index": group_data["id_to_index"],
                        "index_to_id": group_data["index_to_id"]
                    }
                    base_index_name = f'base_{form_type}'
                    group_vector_count = len(group_data["metadata"])
                    cursor.execute("""
                        INSERT INTO rag_vector_index (
                            index_name, form_type, index_data, metadata_json, index_size, vector_count
                        ) VALUES (%s, %s, %s, %s::jsonb, %s, %s)
                        ON CONFLICT (index_name, form_type)
                        DO UPDATE SET
                            index_data = EXCLUDED.index_data,
                            metadata_json = EXCLUDED.metadata_json,
                            index_size = EXCLUDED.index_size,
                            vector_count = EXCLUDED.vector_count,
                            updated_at = CURRENT_TIMESTAMP
                    """, (
                        base_index_name,
                        form_type,
                        index_data_bytes,
                        json.dumps(metadata_json, allow_nan=False),
                        index_size,
                        group_vector_count
                    ))
            if not form_type_groups:
                metadata_json = {
                    "metadata": cleaned_metadata,
                    "id_to_index": self.id_to_index,
                    "index_to_id": {str(k): v for k, v in self.index_to_id.items()}
                }
                with self.db.get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute("""
                        INSERT INTO rag_vector_index (
                            index_name, form_type, index_data, metadata_json, index_size, vector_count
                        ) VALUES (%s, %s, %s, %s::jsonb, %s, %s)
                        ON CONFLICT (index_name, form_type)
                        DO UPDATE SET
                            index_data = EXCLUDED.index_data,
                            metadata_json = EXCLUDED.metadata_json,
                            index_size = EXCLUDED.index_size,
                            vector_count = EXCLUDED.vector_count,
                            updated_at = CURRENT_TIMESTAMP
                    """, (
                        'base',
                        None,
                        index_data_bytes,
                        json.dumps(metadata_json, allow_nan=False),
                        index_size,
                        vector_count
                    ))
        except Exception as e:
            print(f"âš ï¸ DB ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    def add_example(
        self,
        ocr_text: str,
        answer_json: Dict[str, Any],
        metadata: Optional[Dict[str, Any]] = None,
        skip_duplicate: bool = True
    ) -> Optional[str]:
        import uuid
        metadata = metadata or {}
        if skip_duplicate:
            pdf_name = metadata.get('pdf_name')
            page_num = metadata.get('page_num')
            if pdf_name is not None and page_num is not None:
                for existing_id, existing_data in self.metadata.items():
                    existing_metadata = existing_data.get('metadata', {})
                    if (existing_metadata.get('pdf_name') == pdf_name and 
                        existing_metadata.get('page_num') == page_num):
                        return None
        # OCR í…ìŠ¤íŠ¸ ì •ê·œí™” (ë°˜ê° â†’ ì „ê° ë³€í™˜)
        from modules.utils.text_normalizer import normalize_ocr_text
        ocr_text = normalize_ocr_text(ocr_text, use_fullwidth=True)  # ì •ê·œí™”ëœ OCR í…ìŠ¤íŠ¸
        
        doc_id = str(uuid.uuid4())
        model = self._get_embedding_model()
        processed_text = self.preprocess_ocr_text(ocr_text)
        embedding = model.encode([processed_text], convert_to_numpy=True).astype('float32')
        faiss_index = self.index.ntotal
        self.index.add(embedding)
        key_order = self._extract_key_order(answer_json)
        self.metadata[doc_id] = {
            "ocr_text": ocr_text,
            "answer_json": answer_json,
            "metadata": metadata,
            "key_order": key_order
        }
        self.id_to_index[doc_id] = faiss_index
        self.index_to_id[faiss_index] = doc_id
        self._save_index()
        self._refresh_bm25_index()
        return doc_id
    
    def build_shard(
        self,
        shard_pages: List[Dict[str, Any]],
        form_type: Optional[str] = None
    ) -> Optional[Tuple[str, str]]:
        """
        Shard FAISS ì¸ë±ìŠ¤ ìƒì„±
        
        Args:
            shard_pages: í˜ì´ì§€ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
                [{
                    'pdf_name': str,
                    'page_num': int,
                    'ocr_text': str,
                    'answer_json': Dict,
                    'metadata': Dict,
                    'page_key': str,
                    'page_hash': str
                }, ...]
            form_type: ì–‘ì‹ì§€ ë²ˆí˜¸ (01, 02, 03, 04, 05)
            
        Returns:
            (shard_identifier, shard_id) íŠœí”Œ ë˜ëŠ” None
        """
        if not shard_pages:
            return None
        
        import uuid
        import time
        
        # Shard ID ìƒì„±
        shard_id = str(uuid.uuid4())
        timestamp = int(time.time())
        shard_name = f"shard_{timestamp}_{shard_id[:8]}"
        
        # ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ ìƒì„±
        embedding_dim = self._get_embedding_dim()
        shard_index = faiss.IndexFlatL2(embedding_dim)
        shard_metadata = {}
        shard_id_to_index = {}
        shard_index_to_id = {}
        
        # OCR í…ìŠ¤íŠ¸ ì •ê·œí™”
        from modules.utils.text_normalizer import normalize_ocr_text
        model = self._get_embedding_model()
        
        # í˜ì´ì§€ë“¤ì„ ì„ë² ë”©í•˜ì—¬ ì¶”ê°€
        for page_data in shard_pages:
            ocr_text = page_data.get('ocr_text', '')
            answer_json = page_data.get('answer_json', {})
            metadata = page_data.get('metadata', {})
            page_key = page_data.get('page_key', '')
            
            if not ocr_text:
                continue
            
            # OCR í…ìŠ¤íŠ¸ ì •ê·œí™”
            normalized_text = normalize_ocr_text(ocr_text, use_fullwidth=True)
            processed_text = self.preprocess_ocr_text(normalized_text)
            
            # ì„ë² ë”© ìƒì„±
            embedding = model.encode([processed_text], convert_to_numpy=True).astype('float32')
            
            # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
            faiss_idx = shard_index.ntotal
            shard_index.add(embedding)
            
            # doc_id ìƒì„± (page_key ì‚¬ìš©)
            doc_id = page_key if page_key else str(uuid.uuid4())
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            key_order = self._extract_key_order(answer_json)
            shard_metadata[doc_id] = {
                "ocr_text": normalized_text,
                "answer_json": answer_json,
                "metadata": metadata,
                "key_order": key_order
            }
            shard_id_to_index[doc_id] = faiss_idx
            shard_index_to_id[faiss_idx] = doc_id
        
        if shard_index.ntotal == 0:
            return None
        
        # DBì— shard ì €ì¥
        if self.use_db:
            try:
                serialized = faiss.serialize_index(shard_index)
                if hasattr(serialized, 'tobytes'):
                    index_data_bytes = serialized.tobytes()
                else:
                    index_data_bytes = bytes(serialized)
                index_size = len(index_data_bytes)
                vector_count = shard_index.ntotal
                
                def clean_for_json(obj):
                    import math
                    if isinstance(obj, dict):
                        return {k: clean_for_json(v) for k, v in obj.items()}
                    elif isinstance(obj, list):
                        return [clean_for_json(item) for item in obj]
                    elif isinstance(obj, float):
                        if math.isnan(obj) or math.isinf(obj):
                            return None
                        return obj
                    return obj
                
                cleaned_metadata = clean_for_json(shard_metadata)
                metadata_json = {
                    "metadata": cleaned_metadata,
                    "id_to_index": shard_id_to_index,
                    "index_to_id": {str(k): v for k, v in shard_index_to_id.items()}
                }
                
                with self.db.get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute("""
                        INSERT INTO rag_vector_index (
                            index_name, form_type, index_data, metadata_json, index_size, vector_count
                        ) VALUES (%s, %s, %s, %s::jsonb, %s, %s)
                    """, (
                        shard_name,
                        form_type or '',
                        index_data_bytes,
                        json.dumps(metadata_json, allow_nan=False),
                        index_size,
                        vector_count
                    ))
                
                return (shard_name, shard_id)
            except Exception as e:
                print(f"âš ï¸ Shard ì €ì¥ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                return None
        else:
            # íŒŒì¼ ëª¨ë“œ (êµ¬í˜„ í•„ìš” ì‹œ ì¶”ê°€)
            print("âš ï¸ íŒŒì¼ ëª¨ë“œì˜ shard ìƒì„±ì€ ì•„ì§ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return None
    
    def merge_shard(self, shard_identifier: str) -> bool:
        """
        Shardë¥¼ base ì¸ë±ìŠ¤ì— ë³‘í•©
        
        Args:
            shard_identifier: shard ì´ë¦„ (DB ëª¨ë“œ) ë˜ëŠ” íŒŒì¼ ê²½ë¡œ (íŒŒì¼ ëª¨ë“œ)
            
        Returns:
            ë³‘í•© ì„±ê³µ ì—¬ë¶€
        """
        if not self.use_db:
            print("âš ï¸ íŒŒì¼ ëª¨ë“œì˜ shard mergeëŠ” ì•„ì§ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return False
        
        try:
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                
                # Shard ì¡°íšŒ
                cursor.execute("""
                    SELECT index_data, metadata_json, vector_count, form_type
                    FROM rag_vector_index
                    WHERE index_name = %s
                    ORDER BY updated_at DESC
                    LIMIT 1
                """, (shard_identifier,))
                
                row = cursor.fetchone()
                if not row:
                    print(f"âš ï¸ Shardë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {shard_identifier}")
                    return False
                
                shard_data_bytes, shard_metadata_json, shard_vector_count, form_type = row
                
                # Shard ì¸ë±ìŠ¤ ì—­ì§ë ¬í™”
                if isinstance(shard_data_bytes, memoryview):
                    shard_data_bytes = np.frombuffer(shard_data_bytes, dtype=np.uint8)
                elif isinstance(shard_data_bytes, bytes):
                    shard_data_bytes = np.frombuffer(shard_data_bytes, dtype=np.uint8)
                else:
                    shard_data_bytes = np.frombuffer(bytes(shard_data_bytes), dtype=np.uint8)
                
                shard_index = faiss.deserialize_index(shard_data_bytes)
                shard_metadata = shard_metadata_json.get('metadata', {})
                shard_id_to_index = shard_metadata_json.get('id_to_index', {})
                shard_index_to_id_raw = shard_metadata_json.get('index_to_id', {})
                shard_index_to_id = {int(k): v for k, v in shard_index_to_id_raw.items()}
                
                # Base ì¸ë±ìŠ¤ ë¡œë“œ (form_typeë³„)
                base_index_name = f'base_{form_type}' if form_type else 'base'
                cursor.execute("""
                    SELECT index_data, metadata_json, vector_count
                    FROM rag_vector_index
                    WHERE index_name = %s AND (form_type = %s OR (form_type IS NULL AND %s IS NULL))
                    ORDER BY updated_at DESC
                    LIMIT 1
                """, (base_index_name, form_type, form_type))
                
                base_row = cursor.fetchone()
                if base_row:
                    # ê¸°ì¡´ base ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ë¡œë“œ
                    base_data_bytes, base_metadata_json, base_vector_count = base_row
                    if isinstance(base_data_bytes, memoryview):
                        base_data_bytes = np.frombuffer(base_data_bytes, dtype=np.uint8)
                    elif isinstance(base_data_bytes, bytes):
                        base_data_bytes = np.frombuffer(base_data_bytes, dtype=np.uint8)
                    else:
                        base_data_bytes = np.frombuffer(bytes(base_data_bytes), dtype=np.uint8)
                    
                    base_index = faiss.deserialize_index(base_data_bytes)
                    base_metadata = base_metadata_json.get('metadata', {})
                    base_id_to_index = base_metadata_json.get('id_to_index', {})
                    base_index_to_id_raw = base_metadata_json.get('index_to_id', {})
                    base_index_to_id = {int(k): v for k, v in base_index_to_id_raw.items()}
                else:
                    # Base ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                    embedding_dim = self._get_embedding_dim()
                    base_index = faiss.IndexFlatL2(embedding_dim)
                    base_metadata = {}
                    base_id_to_index = {}
                    base_index_to_id = {}
                
                # Shardë¥¼ baseì— ë³‘í•©
                base_vector_count_before = base_index.ntotal
                base_index.merge_from(shard_index)
                
                # ë©”íƒ€ë°ì´í„° ë³‘í•©
                for doc_id, shard_faiss_idx in shard_id_to_index.items():
                    new_faiss_idx = base_vector_count_before + shard_faiss_idx
                    base_metadata[doc_id] = shard_metadata.get(doc_id, {})
                    base_id_to_index[doc_id] = new_faiss_idx
                    base_index_to_id[new_faiss_idx] = doc_id
                
                # Base ì¸ë±ìŠ¤ ì €ì¥
                serialized = faiss.serialize_index(base_index)
                if hasattr(serialized, 'tobytes'):
                    index_data_bytes = serialized.tobytes()
                else:
                    index_data_bytes = bytes(serialized)
                index_size = len(index_data_bytes)
                vector_count = base_index.ntotal
                
                def clean_for_json(obj):
                    import math
                    if isinstance(obj, dict):
                        return {k: clean_for_json(v) for k, v in obj.items()}
                    elif isinstance(obj, list):
                        return [clean_for_json(item) for item in obj]
                    elif isinstance(obj, float):
                        if math.isnan(obj) or math.isinf(obj):
                            return None
                        return obj
                    return obj
                
                cleaned_metadata = clean_for_json(base_metadata)
                metadata_json = {
                    "metadata": cleaned_metadata,
                    "id_to_index": base_id_to_index,
                    "index_to_id": {str(k): v for k, v in base_index_to_id.items()}
                }
                
                cursor.execute("""
                    INSERT INTO rag_vector_index (
                        index_name, form_type, index_data, metadata_json, index_size, vector_count
                    ) VALUES (%s, %s, %s, %s::jsonb, %s, %s)
                    ON CONFLICT (index_name, form_type)
                    DO UPDATE SET
                        index_data = EXCLUDED.index_data,
                        metadata_json = EXCLUDED.metadata_json,
                        index_size = EXCLUDED.index_size,
                        vector_count = EXCLUDED.vector_count,
                        updated_at = CURRENT_TIMESTAMP
                """, (
                    base_index_name,
                    form_type or None,
                    index_data_bytes,
                    json.dumps(metadata_json, allow_nan=False),
                    index_size,
                    vector_count
                ))
                
                # Shard ì‚­ì œ
                cursor.execute("""
                    DELETE FROM rag_vector_index
                    WHERE index_name = %s
                """, (shard_identifier,))
                
                return True
                
        except Exception as e:
            print(f"âš ï¸ Shard merge ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _extract_key_order(self, json_data: Dict[str, Any]) -> Dict[str, Any]:
        if not json_data or not isinstance(json_data, dict):
            return {"page_keys": [], "item_keys": []}
        key_order = {
            "page_keys": list(json_data.keys()),
            "item_keys": []
        }
        if "items" in json_data and isinstance(json_data["items"], list) and len(json_data["items"]) > 0:
            first_item = json_data["items"][0]
            if isinstance(first_item, dict):
                key_order["item_keys"] = list(first_item.keys())
        return key_order
    
    def get_key_order_by_form_type(self, form_type: Optional[str] = None) -> Optional[Dict[str, Any]]:
        if self.metadata:
            # item_keysê°€ ìˆëŠ” key_orderë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¾ê¸°
            best_key_order = None
            fallback_key_order = None
            
            for doc_id, data in self.metadata.items():
                metadata_info = data.get("metadata", {})
                if form_type:
                    if metadata_info.get("form_type") == form_type:
                        key_order = data.get("key_order")
                        if key_order:
                            # item_keysê°€ ìˆëŠ” key_orderë¥¼ ìš°ì„  ì„ íƒ
                            item_keys = key_order.get("item_keys", [])
                            if item_keys and len(item_keys) > 0:
                                best_key_order = key_order
                                break  # item_keysê°€ ìˆëŠ” ê²ƒì„ ì°¾ìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜
                            elif fallback_key_order is None:
                                fallback_key_order = key_order
                else:
                    key_order = data.get("key_order")
                    if key_order:
                        item_keys = key_order.get("item_keys", [])
                        if item_keys and len(item_keys) > 0:
                            best_key_order = key_order
                            break
                        elif fallback_key_order is None:
                            fallback_key_order = key_order
            
            # item_keysê°€ ìˆëŠ” key_orderë¥¼ ìš°ì„  ë°˜í™˜
            if best_key_order:
                return best_key_order
            
            # ì—†ìœ¼ë©´ fallback ë°˜í™˜
            if fallback_key_order:
                return fallback_key_order
        if self.use_db and form_type:
            try:
                with self.db.get_connection() as conn:
                    cursor = conn.cursor()
                    for index_name_pattern in [f'base_{form_type}', 'shard_%']:
                        if index_name_pattern.startswith('base_'):
                            cursor.execute("""
                                SELECT metadata_json
                                FROM rag_vector_index
                                WHERE index_name = %s AND form_type = %s
                                ORDER BY updated_at DESC
                                LIMIT 1
                            """, (index_name_pattern, form_type))
                        else:
                            cursor.execute("""
                                SELECT metadata_json
                                FROM rag_vector_index
                                WHERE index_name LIKE %s AND form_type = %s
                                ORDER BY updated_at DESC
                                LIMIT 1
                            """, (index_name_pattern, form_type))
                        row = cursor.fetchone()
                        if row:
                            metadata_json = row[0]
                            if isinstance(metadata_json, str):
                                import json
                                try:
                                    metadata_json = json.loads(metadata_json)
                                except Exception:
                                    continue
                            if isinstance(metadata_json, dict):
                                metadata_dict = metadata_json.get('metadata', {})
                                if not metadata_dict:
                                    continue
                                
                                # item_keysê°€ ìˆëŠ” key_orderë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¾ê¸°
                                best_key_order = None
                                fallback_key_order = None
                                
                                for doc_id, data in metadata_dict.items():
                                    if not isinstance(data, dict):
                                        continue
                                    metadata_info = data.get("metadata", {})
                                    if isinstance(metadata_info, dict):
                                        actual_form_type = metadata_info.get("form_type")
                                        if (actual_form_type == form_type or 
                                            str(actual_form_type) == str(form_type) or
                                            (isinstance(actual_form_type, int) and str(actual_form_type).zfill(2) == form_type) or
                                            (isinstance(form_type, str) and actual_form_type == form_type.zfill(2) if isinstance(actual_form_type, int) else False)):
                                            key_order = data.get("key_order")
                                            if key_order:
                                                # item_keysê°€ ìˆëŠ” key_orderë¥¼ ìš°ì„  ì„ íƒ
                                                item_keys = key_order.get("item_keys", [])
                                                if item_keys and len(item_keys) > 0:
                                                    best_key_order = key_order
                                                    break  # item_keysê°€ ìˆëŠ” ê²ƒì„ ì°¾ìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜
                                                elif fallback_key_order is None:
                                                    fallback_key_order = key_order
                                
                                # item_keysê°€ ìˆëŠ” key_orderë¥¼ ìš°ì„  ë°˜í™˜
                                if best_key_order:
                                    return best_key_order
                                
                                # ì—†ìœ¼ë©´ fallback ë°˜í™˜
                                if fallback_key_order:
                                    return fallback_key_order
                                
                                # form_type ë§¤ì¹­ì´ ì•ˆë˜ë©´ item_keysê°€ ìˆëŠ” ì²« ë²ˆì§¸ key_order ì°¾ê¸°
                                for doc_id, data in metadata_dict.items():
                                    if not isinstance(data, dict):
                                        continue
                                    key_order = data.get("key_order")
                                    if key_order:
                                        item_keys = key_order.get("item_keys", [])
                                        if item_keys and len(item_keys) > 0:
                                            return key_order
                                        elif fallback_key_order is None:
                                            fallback_key_order = key_order
                                
                                # ìµœì¢…ì ìœ¼ë¡œ fallback ë°˜í™˜
                                if fallback_key_order:
                                    return fallback_key_order
            except Exception as e:
                print(f"  âš ï¸ [í‚¤ ìˆœì„œ ì¡°íšŒ] DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
                import traceback
                print(f"  ìƒì„¸: {traceback.format_exc()}")
        return None
    
    def get_all_examples(self) -> List[Dict[str, Any]]:
        examples = []
        for doc_id, data in self.metadata.items():
            examples.append({
                "id": doc_id,
                "ocr_text": data.get("ocr_text", ""),
                "answer_json": data.get("answer_json", {}),
                "metadata": data.get("metadata", {}),
                "key_order": data.get("key_order")
            })
        return examples
    
    def count_examples(self) -> int:
        if self.use_db:
            try:
                with self.db.get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT vector_count
                        FROM rag_vector_index
                        WHERE index_name = 'base'
                        ORDER BY updated_at DESC
                        LIMIT 1
                    """)
                    row = cursor.fetchone()
                    if row and row[0]:
                        return row[0]
                    cursor.execute("""
                        SELECT COALESCE(SUM(vector_count), 0)
                        FROM rag_vector_index
                    """)
                    row = cursor.fetchone()
                    if row and row[0]:
                        return row[0]
                    return len(self.metadata)
            except Exception as e:
                print(f"âš ï¸ DBì—ì„œ ë²¡í„° ìˆ˜ í™•ì¸ ì‹¤íŒ¨: {e}")
                return len(self.metadata)
        else:
            return len(self.metadata)
    
    @staticmethod
    def preprocess_ocr_text(ocr_text: str) -> str:
        import re
        text = re.sub(r'\s+', ' ', ocr_text)
        text = re.sub(r'(\d+),(\d+)', r'\1\2', text)
        text = re.sub(r'\n+', ' ', text)
        return text.strip()
    
    @staticmethod
    def _tokenize(text: str) -> List[str]:
        import re
        tokens = re.findall(r'\b\w+\b|[ê°€-í£]+|[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠ]+|[ä¸€-é¾ ]+', text)
        return tokens
    
    def _build_bm25_index(self):
        try:
            from rank_bm25 import BM25Okapi
        except ImportError:
            self._bm25_index = None
            return
        if self._bm25_index is not None:
            return
        all_examples = self.get_all_examples()
        if not all_examples:
            self._bm25_index = None
            return
        self._bm25_texts = []
        self._bm25_example_map = {}
        for example in all_examples:
            ocr_text = example.get("ocr_text", "")
            doc_id = example.get("id", "")
            if not doc_id:
                continue
            preprocessed = self.preprocess_ocr_text(ocr_text)
            tokens = self._tokenize(preprocessed)
            if tokens:
                self._bm25_texts.append(tokens)
                self._bm25_example_map[doc_id] = len(self._bm25_texts) - 1
        if self._bm25_texts:
            self._bm25_index = BM25Okapi(self._bm25_texts)
        else:
            self._bm25_index = None
    
    def _refresh_bm25_index(self):
        self._bm25_index = None
        self._bm25_texts = None
        self._bm25_example_map = None
        self._build_bm25_index()
    
    def _create_search_result(
        self,
        doc_id: str,
        data: Dict[str, Any],
        similarity: float,
        distance: float,
        source: str
    ) -> Dict[str, Any]:
        return {
            "ocr_text": data.get("ocr_text", ""),
            "answer_json": data.get("answer_json", {}),
            "metadata": data.get("metadata", {}),
            "key_order": data.get("key_order"),
            "similarity": similarity,
            "distance": float(distance),
            "id": doc_id,
            "source": source
        }
    
    def _normalize_score(self, score: float, min_score: float, max_score: float) -> float:
        if max_score > min_score:
            return (score - min_score) / (max_score - min_score)
        elif max_score == min_score and max_score > 0:
            return 1.0
        else:
            return 0.0
    
    def search_vector_only(
        self,
        query_text: str,
        top_k: int = 3,
        similarity_threshold: float = 0.7,
        form_type: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        processed_query = self.preprocess_ocr_text(query_text)
        model = self._get_embedding_model()
        query_embedding = model.encode([processed_query], convert_to_numpy=True).astype('float32')
        all_results = []
        if form_type and self.use_db:
            # print(f"ğŸ” [RAG ê²€ìƒ‰] ì–‘ì‹ì§€ {form_type}ì˜ ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
            index, metadata, id_to_index, index_to_id = self._load_index_from_db(form_type=form_type)
            if index is None:
                print(f"âš ï¸ RAG ê²€ìƒ‰: ì–‘ì‹ì§€ {form_type}ì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            # print(f"ğŸ” [RAG ê²€ìƒ‰] ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: ntotal={index.ntotal}, ë©”íƒ€ë°ì´í„°={len(metadata)}ê°œ")
        else:
            index = self.index
            metadata = self.metadata
            id_to_index = self.id_to_index
            index_to_id = self.index_to_id
        if index is None:
            print(f"âš ï¸ RAG ê²€ìƒ‰: ì¸ë±ìŠ¤ê°€ Noneì…ë‹ˆë‹¤. ë²¡í„° DBê°€ ì œëŒ€ë¡œ ë¡œë“œë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return []
        if index.ntotal > 0:
            k = min(top_k * 2, index.ntotal)
            distances, indices = index.search(query_embedding, k)
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                if idx == -1:
                    continue
                doc_id = index_to_id.get(idx)
                if not doc_id:
                    continue
                similarity = max(0.0, 1.0 - (distance / 100.0))
                if similarity < similarity_threshold:
                    continue
                data = metadata.get(doc_id, {})
                page_metadata = data.get("metadata", {})
                if page_metadata.get("status") == "deleted":
                    continue
                all_results.append(self._create_search_result(doc_id, data, similarity, distance, "base"))
        else:
            pass
            print(f"âš ï¸ RAG ê²€ìƒ‰: ì¸ë±ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. (ntotal={index.ntotal}, ë©”íƒ€ë°ì´í„°={len(metadata)}ê°œ)")
        
        # ìœ ì‚¬ë„ë¡œ ì •ë ¬ ë° ì¤‘ë³µ ì œê±° (doc_id ê¸°ì¤€)
        seen_doc_ids = set()
        unique_results = []
        for result in sorted(all_results, key=lambda x: x["similarity"], reverse=True):
            doc_id = result["id"]
            if doc_id not in seen_doc_ids:
                seen_doc_ids.add(doc_id)
                unique_results.append(result)
        
        return unique_results[:top_k]
    
    def search_hybrid(
        self,
        query_text: str,
        top_k: int = 3,
        similarity_threshold: float = 0.7,
        hybrid_alpha: float = 0.5,
        form_type: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: BM25 + ë²¡í„° ê²€ìƒ‰ ê²°í•©
        
        Args:
            query_text: ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ (OCR í…ìŠ¤íŠ¸)
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            similarity_threshold: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0 ~ 1.0)
            hybrid_alpha: í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜ (0.0~1.0, 0.5 = ë²¡í„°ì™€ BM25 ë™ì¼ ê°€ì¤‘ì¹˜)
            form_type: ì–‘ì‹ì§€ ë²ˆí˜¸ (01, 02, 03, 04, 05). Noneì´ë©´ ëª¨ë“  ì–‘ì‹ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (hybrid_score í¬í•¨)
        """
        # BM25 ì¸ë±ìŠ¤ êµ¬ì¶• (form_typeë³„ë¡œ í•„í„°ë§ í•„ìš”í•˜ì§€ë§Œ, í˜„ì¬ëŠ” ì „ì²´ ì¸ë±ìŠ¤ ì‚¬ìš©)
        # TODO: form_typeë³„ BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
        self._build_bm25_index()
        
        if self._bm25_index is None:
            # BM25ê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•˜ë©´ ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš©
            return self.search_vector_only(query_text, top_k, similarity_threshold, form_type)
        
        # ë²¡í„° ê²€ìƒ‰ (ë” ë§ì€ í›„ë³´)
        vector_results = self.search_vector_only(
            query_text, top_k * 3, 0.0, form_type  # threshold ë¬´ì‹œ
        )
        
        # BM25 ê²€ìƒ‰
        processed_query = self.preprocess_ocr_text(query_text)
        query_tokens = self._tokenize(processed_query)
        
        if not query_tokens:
            return self.search_vector_only(query_text, top_k, similarity_threshold, form_type)
        
        bm25_scores_list = self._bm25_index.get_scores(query_tokens)
        
        # doc_id -> BM25 ì ìˆ˜ ë§¤í•‘
        bm25_scores = {}
        for doc_id, bm25_idx in self._bm25_example_map.items():
            if bm25_idx < len(bm25_scores_list):
                bm25_scores[doc_id] = bm25_scores_list[bm25_idx]
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        hybrid_results = []
        candidate_bm25_scores = [bm25_scores.get(r["id"], 0.0) for r in vector_results]
        
        if candidate_bm25_scores:
            max_bm25 = max(candidate_bm25_scores)
            min_bm25 = min(candidate_bm25_scores)
        else:
            max_bm25 = 1.0
            min_bm25 = 0.0
        
        for result in vector_results:
            doc_id = result["id"]
            vector_similarity = result["similarity"]
            
            # BM25 ì ìˆ˜ ì •ê·œí™”
            bm25_score = bm25_scores.get(doc_id, 0.0)
            normalized_bm25 = self._normalize_score(bm25_score, min_bm25, max_bm25)
            
            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜
            hybrid_score = hybrid_alpha * vector_similarity + (1 - hybrid_alpha) * normalized_bm25
            
            # ë²¡í„° ìœ ì‚¬ë„ê°€ thresholdë¥¼ í†µê³¼í•˜ë©´ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ì™€ ê´€ê³„ì—†ì´ í¬í•¨
            # (BM25 ì ìˆ˜ê°€ ë‚®ì•„ë„ ë²¡í„° ìœ ì‚¬ë„ê°€ ë†’ìœ¼ë©´ ìœ ì§€)
            if hybrid_score < similarity_threshold and vector_similarity < similarity_threshold:
                continue
            
            result["bm25_score"] = normalized_bm25
            result["hybrid_score"] = hybrid_score
            hybrid_results.append(result)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ì •ë ¬
        hybrid_results.sort(key=lambda x: x["hybrid_score"], reverse=True)
        
        return hybrid_results[:top_k]
    
    def search_similar_advanced(
        self,
        query_text: str,
        top_k: int = 3,
        similarity_threshold: float = 0.7,
        search_method: str = "vector",  # "vector", "hybrid"
        hybrid_alpha: float = 0.5,
        form_type: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        í†µí•© ê²€ìƒ‰ í•¨ìˆ˜ (ê²€ìƒ‰ ë°©ì‹ ì„ íƒ ê°€ëŠ¥)
        
        Args:
            query_text: ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ (OCR í…ìŠ¤íŠ¸)
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            similarity_threshold: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0 ~ 1.0)
            search_method: ê²€ìƒ‰ ë°©ì‹ ("vector", "hybrid")
            hybrid_alpha: í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜ (hybrid ë°©ì‹ ì‚¬ìš© ì‹œ)
            form_type: ì–‘ì‹ì§€ ë²ˆí˜¸ (01, 02, 03, 04, 05). Noneì´ë©´ ëª¨ë“  ì–‘ì‹ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if search_method == "hybrid":
            return self.search_hybrid(
                query_text, top_k, similarity_threshold, hybrid_alpha, form_type
            )
        else:  # "vector" ë˜ëŠ” ê¸°ë³¸ê°’
            return self.search_vector_only(
                query_text, top_k, similarity_threshold, form_type
            )


# ì „ì—­ RAG Manager ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_rag_manager: Optional[RAGManager] = None
_rag_manager_lock = Lock()  # ì‹±ê¸€í†¤ ìƒì„± ë½


def get_rag_manager(use_db: bool = True) -> RAGManager:
    """
    ì „ì—­ RAG Manager ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ìŠ¤ë ˆë“œ ì•ˆì „)
    
    Args:
        use_db: Trueë©´ DBì— ì €ì¥, Falseë©´ ë¡œì»¬ íŒŒì¼ì— ì €ì¥ (ê¸°ë³¸ê°’: True)
    
    Returns:
        RAGManager ì¸ìŠ¤í„´ìŠ¤
    """
    global _rag_manager
    if _rag_manager is None:
        with _rag_manager_lock:
            # ì´ì¤‘ ì²´í¬
            if _rag_manager is None:
                _rag_manager = RAGManager(use_db=use_db)
    return _rag_manager


